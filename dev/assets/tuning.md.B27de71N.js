import{_ as o,c as t,o as a,aA as i}from"./chunks/framework.DyAPhMiC.js";const p=JSON.parse('{"title":"Tuning and Workflow Tips","description":"","frontmatter":{},"headers":[],"relativePath":"tuning.md","filePath":"tuning.md","lastUpdated":1769901654000}'),s={name:"tuning.md"};function n(r,e,l,c,d,u){return a(),t("div",null,[...e[0]||(e[0]=[i('<h1 id="tuning-and-workflow-tips" tabindex="-1">Tuning and Workflow Tips <a class="header-anchor" href="#tuning-and-workflow-tips" aria-label="Permalink to &quot;Tuning and Workflow Tips&quot;">​</a></h1><p>I give a short guide below on how I like to tune PySR for my applications.</p><p>First, my general tips would be to avoid using redundant operators, like how <code>pow</code> can do the same things as <code>square</code>, or how <code>-</code> (binary) and <code>neg</code> (unary) are equivalent. The fewer operators the better! Only use operators you need.</p><p>When running PySR, I usually do the following:</p><p>I run from IPython (Jupyter Notebooks don&#39;t work as well<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>) on the head node of a slurm cluster. Passing <code>cluster_manager=&quot;slurm&quot;</code> will make PySR set up a run over the entire allocation. I set <code>procs</code> equal to the total number of cores over my entire allocation.</p><p>I use the <a href="https://ai.damtp.cam.ac.uk/pysr/examples/#12-using-tensorboard-for-logging" target="_blank" rel="noreferrer">tensorboard feature</a> for experiment tracking.</p><ol><li>I start by using the default parameters.</li><li>I use only the operators I think it needs and no more.</li><li>Increase <code>populations</code> to <code>3*num_cores</code>.</li><li>If my dataset is more than 1000 points, I either subsample it (low-dimensional and not much noise) or set <code>batching=True</code> (high-dimensional or very noisy, so it needs to evaluate on all the data).</li><li>While on a laptop or single node machine, you might leave the default <code>ncycles_per_iteration</code>, on a cluster with ~100 cores I like to set <code>ncycles_per_iteration</code> to maybe <code>5000</code> or so, until the head node occupation is under <code>10%</code>. (A larger value means the workers talk less frequently to eachother, which is useful when you have many workers!)</li><li>Set <code>constraints</code> and <code>nested_constraints</code> as strict as possible. These can help quite a bit with exploration. Typically, if I am using <code>pow</code>, I would set <code>constraints={&quot;pow&quot;: (9, 1)}</code>, so that power laws can only have a variable or constant as their exponent. If I am using <code>sin</code> and <code>cos</code>, I also like to set <code>nested_constraints={&quot;sin&quot;: {&quot;sin&quot;: 0, &quot;cos&quot;: 0}, &quot;cos&quot;: {&quot;sin&quot;: 0, &quot;cos&quot;: 0}}</code>, so that sin and cos can&#39;t be nested, which seems to happen frequently. (Although in practice I would just use <code>sin</code>, since the search could always add a phase offset!)</li><li>Set <code>maxsize</code> a bit larger than the final size you want. e.g., if you want a final equation of size <code>30</code>, you might set this to <code>35</code>, so that it has a bit of room to explore.</li><li>I typically don&#39;t use <code>maxdepth</code>, but if I do, I set it strictly, while also leaving a bit of room for exploration. e.g., if you want a final equation limited to a depth of <code>5</code>, you might set this to <code>6</code> or <code>7</code>, so that it has a bit of room to explore.</li><li>Set <code>parsimony</code> equal to about the minimum loss you would expect, divided by 5-10. e.g., if you expect the final equation to have a loss of <code>0.001</code>, you might set <code>parsimony=0.0001</code>.</li><li>Set <code>weight_optimize</code> to some larger value, maybe <code>0.001</code>. This is very important if <code>ncycles_per_iteration</code> is large, so that optimization happens more frequently.</li><li>Set <code>turbo</code> to <code>True</code>. This turns on advanced loop vectorization, but is still quite experimental. It should give you a nice 20% or more speedup.</li><li>For final runs, after I have tuned everything, I typically set <code>niterations</code> to some very large value, and just let it run for a week until my job finishes (genetic algorithms tend not to converge, they can look like they settle down, but then find a new family of expression, and explore a new space). If I am satisfied with the current equations (which are visible either in the terminal or in the saved csv file), I quit the job early.</li></ol><p>Since I am running in IPython, I can just hit <code>q</code> and then <code>&lt;enter&gt;</code> to stop the job, tweak the hyperparameters, and then start the search again. I can also use <code>warm_start=True</code> if I wish to continue where I left off (though note that changing some parameters, like <code>maxsize</code>, are incompatible with warm starts).</p><p>Some things I try out to see if they help:</p><ol><li>Play around with <code>complexity_of_operators</code>. Set operators you dislike (e.g., <code>pow</code>) to have a larger complexity.</li><li>Try setting <code>adaptive_parsimony_scaling</code> a bit larger, maybe up to <code>1000</code>.</li><li>Sometimes I try using <code>warmup_maxsize_by</code>. This is useful if you find that the search finds a very complex equation very quickly, and then gets stuck. It basically forces it to start at the simpler equations and build up complexity slowly.</li><li>Play around with different losses: <ul><li>I typically try <code>L2DistLoss()</code> and <code>L1DistLoss()</code>. L1 loss is more robust to outliers compared to L2 (L1 finds the median, while L2 finds the mean of a random variable), so is often a good choice for a noisy dataset.</li><li>I might also provide the <code>weights</code> parameter to <code>fit</code> if there is some reasonable choice of weighting. For example, maybe I know the signal-to-noise of a particular row of <code>y</code> - I would set that SNR equal to the weights. Or, perhaps I do some sort of importance sampling, and weight the rows by importance.</li></ul></li></ol><p>Very rarely I might also try tuning the mutation weights, the crossover probability, or the optimization parameters. I never use <code>denoise</code> or <code>select_k_features</code> as I find they aren&#39;t very useful.</p><p>For large datasets I usually just randomly sample ~1000 points or so. In case all the points matter, I might use <code>batching=True</code>.</p><p>If I find the equations get very complex and I&#39;m not sure if they are numerically precise, I might set <code>precision=64</code>.</p><p>Once a run is finished, I use the <code>PySRRegressor.from_file</code> function to load the saved search in a different process (requires the pickle file, and possibly also the <code>.csv</code> file if you quit early). I can then explore the equations, convert them to LaTeX, and plot their output.</p><h2 id="more-tips" tabindex="-1">More Tips <a class="header-anchor" href="#more-tips" aria-label="Permalink to &quot;More Tips&quot;">​</a></h2><p>You might also wish to explore the <a href="https://github.com/MilesCranmer/PySR/discussions/" target="_blank" rel="noreferrer">discussions</a> page for more tips, and to see if anyone else has had similar questions. Be sure to also read through the <a href="./api">reference</a>.</p><hr class="footnotes-sep"><section class="footnotes"><ol class="footnotes-list"><li id="fn1" class="footnote-item"><p>Jupyter Notebooks are supported by PySR, but miss out on some useful features available in IPython and Python: the progress bar, and early stopping with &quot;q&quot;. In Jupyter you cannot interrupt a search once it has started; you have to restart the kernel. See <a href="https://github.com/MilesCranmer/PySR/issues/260" target="_blank" rel="noreferrer">this issue</a> for updates. <a href="#fnref1" class="footnote-backref">↩︎</a></p></li></ol></section>',18)])])}const m=o(s,[["render",n]]);export{p as __pageData,m as default};

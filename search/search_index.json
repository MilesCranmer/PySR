{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>PySR searches for symbolic expressions which optimize a particular objective.</p> <p>https://github.com/MilesCranmer/PySR/assets/7593028/c8511a49-b408-488f-8f18-b1749078268f</p>"},{"location":"#pysr-high-performance-symbolic-regression-in-python-and-julia","title":"PySR: High-Performance Symbolic Regression in Python and Julia","text":"Docs Forums Paper colab demo pip conda Stats <p>If you find PySR useful, please cite the paper arXiv:2305.01582. If you've finished a project with PySR, please submit a PR to showcase your work on the research showcase page!</p> <p>Contents:</p> <ul> <li>Why PySR?</li> <li>Installation</li> <li>Quickstart</li> <li>\u2192 Documentation</li> <li>Contributors</li> </ul> <p>Test status</p> Linux Windows macOS Docker Conda Coverage"},{"location":"#why-pysr","title":"Why PySR?","text":"<p>PySR is an open-source tool for Symbolic Regression: a machine learning task where the goal is to find an interpretable symbolic expression that optimizes some objective.</p> <p>Over a period of several years, PySR has been engineered from the ground up to be (1) as high-performance as possible, (2) as configurable as possible, and (3) easy to use. PySR is developed alongside the Julia library SymbolicRegression.jl, which forms the powerful search engine of PySR. The details of these algorithms are described in the PySR paper.</p> <p>Symbolic regression works best on low-dimensional datasets, but one can also extend these approaches to higher-dimensional spaces by using \"Symbolic Distillation\" of Neural Networks, as explained in 2006.11287, where we apply it to N-body problems. Here, one essentially uses symbolic regression to convert a neural net to an analytic equation. Thus, these tools simultaneously present an explicit and powerful way to interpret deep neural networks.</p>"},{"location":"#installation","title":"Installation","text":""},{"location":"#pip","title":"Pip","text":"<p>You can install PySR with pip:</p> <pre><code>pip install pysr\n</code></pre> <p>Julia dependencies will be installed at first import.</p>"},{"location":"#conda","title":"Conda","text":"<p>Similarly, with conda:</p> <pre><code>conda install -c conda-forge pysr\n</code></pre>   ### Docker    You can also use the `Dockerfile` to install PySR in a docker container  1. Clone this repo. 2. Within the repo's directory, build the docker container: <pre><code>docker build -t pysr .\n</code></pre> 3. You can then start the container with an IPython execution with: <pre><code>docker run -it --rm pysr ipython\n</code></pre>  For more details, see the [docker section](#docker).     ### Apptainer    If you are using PySR on a cluster where you do not have root access, you can use [Apptainer](https://apptainer.org/) to build a container instead of Docker. The `Apptainer.def` file is analogous to the `Dockerfile`, and can be built with:  <pre><code>apptainer build --notest pysr.sif Apptainer.def\n</code></pre>  and launched with  <pre><code>apptainer run pysr.sif\n</code></pre>   ### Troubleshooting    One issue you might run into can result in a hard crash at import with a message like \"`GLIBCXX_...` not found\". This is due to another one of the Python dependencies loading an incorrect `libstdc++` library. To fix this, you should modify your `LD_LIBRARY_PATH` variable to reference the Julia libraries. For example, if the Julia version of `libstdc++.so` is located in `$HOME/.julia/juliaup/julia-1.10.0+0.x64.linux.gnu/lib/julia/` (which likely differs on your system!), you could add:  <pre><code>export LD_LIBRARY_PATH=$HOME/.julia/juliaup/julia-1.10.0+0.x64.linux.gnu/lib/julia/:$LD_LIBRARY_PATH\n</code></pre>  to your `.bashrc` or `.zshrc` file."},{"location":"#quickstart","title":"Quickstart","text":"<p>You might wish to try the interactive tutorial here, which uses the notebook in <code>examples/pysr_demo.ipynb</code>.</p> <p>In practice, I highly recommend using IPython rather than Jupyter, as the printing is much nicer. Below is a quick demo here which you can paste into a Python runtime. First, let's import numpy to generate some test data:</p> <pre><code>import numpy as np\n\nX = 2 * np.random.randn(100, 5)\ny = 2.5382 * np.cos(X[:, 3]) + X[:, 0] ** 2 - 0.5\n</code></pre> <p>We have created a dataset with 100 datapoints, with 5 features each. The relation we wish to model is \\(2.5382 \\cos(x_3) + x_0^2 - 0.5\\).</p> <p>Now, let's create a PySR model and train it. PySR's main interface is in the style of scikit-learn:</p> <pre><code>from pysr import PySRRegressor\n\nmodel = PySRRegressor(\n    niterations=40,  # &lt; Increase me for better results\n    binary_operators=[\"+\", \"*\"],\n    unary_operators=[\n        \"cos\",\n        \"exp\",\n        \"sin\",\n        \"inv(x) = 1/x\",\n        # ^ Custom operator (julia syntax)\n    ],\n    extra_sympy_mappings={\"inv\": lambda x: 1 / x},\n    # ^ Define operator for SymPy as well\n    elementwise_loss=\"loss(prediction, target) = (prediction - target)^2\",\n    # ^ Custom loss function (julia syntax)\n)\n</code></pre> <p>This will set up the model for 40 iterations of the search code, which contains hundreds of thousands of mutations and equation evaluations.</p> <p>Let's train this model on our dataset:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>Internally, this launches a Julia process which will do a multithreaded search for equations to fit the dataset.</p> <p>Equations will be printed during training, and once you are satisfied, you may quit early by hitting 'q' and then \\&lt;enter&gt;.</p> <p>After the model has been fit, you can run <code>model.predict(X)</code> to see the predictions on a given dataset using the automatically-selected expression, or, for example, <code>model.predict(X, 3)</code> to see the predictions of the 3rd equation.</p> <p>You may run:</p> <pre><code>print(model)\n</code></pre> <p>to print the learned equations:</p> <pre><code>PySRRegressor.equations_ = [\n       pick     score                                           equation       loss  complexity\n    0        0.000000                                          4.4324794  42.354317           1\n    1        1.255691                                          (x0 * x0)   3.437307           3\n    2        0.011629                          ((x0 * x0) + -0.28087974)   3.358285           5\n    3        0.897855                              ((x0 * x0) + cos(x3))   1.368308           6\n    4        0.857018                ((x0 * x0) + (cos(x3) * 2.4566472))   0.246483           8\n    5  &gt;&gt;&gt;&gt;       inf  (((cos(x3) + -0.19699033) * 2.5382123) + (x0 *...   0.000000          10\n]\n</code></pre> <p>This arrow in the <code>pick</code> column indicates which equation is currently selected by your <code>model_selection</code> strategy for prediction. (You may change <code>model_selection</code> after <code>.fit(X, y)</code> as well.)</p> <p><code>model.equations_</code> is a pandas DataFrame containing all equations, including callable format (<code>lambda_format</code>), SymPy format (<code>sympy_format</code> - which you can also get with <code>model.sympy()</code>), and even JAX and PyTorch format (both of which are differentiable - which you can get with <code>model.jax()</code> and <code>model.pytorch()</code>).</p> <p>Note that <code>PySRRegressor</code> stores the state of the last search, and will restart from where you left off the next time you call <code>.fit()</code>, assuming you have set <code>warm_start=True</code>. This will cause problems if significant changes are made to the search parameters (like changing the operators). You can run <code>model.reset()</code> to reset the state.</p> <p>You will notice that PySR will save two files: <code>hall_of_fame...csv</code> and <code>hall_of_fame...pkl</code>. The csv file is a list of equations and their losses, and the pkl file is a saved state of the model. You may load the model from the <code>pkl</code> file with:</p> <pre><code>model = PySRRegressor.from_file(\"hall_of_fame.2022-08-10_100832.281.pkl\")\n</code></pre> <p>There are several other useful features such as denoising (e.g., <code>denoise=True</code>), feature selection (e.g., <code>select_k_features=3</code>). For examples of these and other features, see the examples page. For a detailed look at more options, see the options page. You can also see the full API at this page. There are also tips for tuning PySR on this page.</p>"},{"location":"#detailed-example","title":"Detailed Example","text":"<p>The following code makes use of as many PySR features as possible. Note that is just a demonstration of features and you should not use this example as-is. For details on what each parameter does, check out the API page.</p> <pre><code>model = PySRRegressor(\n    procs=4,\n    populations=8,\n    # ^ 2 populations per core, so one is always running.\n    population_size=50,\n    ncycles_per_iteration=500,\n    # ^ Generations between migrations.\n    niterations=10000000,  # Run forever\n    early_stop_condition=(\n        \"stop_if(loss, complexity) = loss &lt; 1e-6 &amp;&amp; complexity &lt; 10\"\n        # Stop early if we find a good and simple equation\n    ),\n    timeout_in_seconds=60 * 60 * 24,\n    # ^ Alternatively, stop after 24 hours have passed.\n    maxsize=50,\n    # ^ Allow greater complexity.\n    maxdepth=10,\n    # ^ But, avoid deep nesting.\n    binary_operators=[\"*\", \"+\", \"-\", \"/\"],\n    unary_operators=[\"square\", \"cube\", \"exp\", \"cos2(x)=cos(x)^2\"],\n    constraints={\n        \"/\": (-1, 9),\n        \"square\": 9,\n        \"cube\": 9,\n        \"exp\": 9,\n    },\n    # ^ Limit the complexity within each argument.\n    # \"inv\": (-1, 9) states that the numerator has no constraint,\n    # but the denominator has a max complexity of 9.\n    # \"exp\": 9 simply states that `exp` can only have\n    # an expression of complexity 9 as input.\n    nested_constraints={\n        \"square\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n        \"cube\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n        \"exp\": {\"square\": 1, \"cube\": 1, \"exp\": 0},\n    },\n    # ^ Nesting constraints on operators. For example,\n    # \"square(exp(x))\" is not allowed, since \"square\": {\"exp\": 0}.\n    complexity_of_operators={\"/\": 2, \"exp\": 3},\n    # ^ Custom complexity of particular operators.\n    complexity_of_constants=2,\n    # ^ Punish constants more than variables\n    select_k_features=4,\n    # ^ Train on only the 4 most important features\n    progress=True,\n    # ^ Can set to false if printing to a file.\n    weight_randomize=0.1,\n    # ^ Randomize the tree much more frequently\n    cluster_manager=None,\n    # ^ Can be set to, e.g., \"slurm\", to run a slurm\n    # cluster. Just launch one script from the head node.\n    precision=64,\n    # ^ Higher precision calculations.\n    warm_start=True,\n    # ^ Start from where left off.\n    turbo=True,\n    # ^ Faster evaluation (experimental)\n    extra_sympy_mappings={\"cos2\": lambda x: sympy.cos(x)**2},\n    # extra_torch_mappings={sympy.cos: torch.cos},\n    # ^ Not needed as cos already defined, but this\n    # is how you define custom torch operators.\n    # extra_jax_mappings={sympy.cos: \"jnp.cos\"},\n    # ^ For JAX, one passes a string.\n)\n</code></pre>"},{"location":"#docker","title":"Docker","text":"<p>You can also test out PySR in Docker, without installing it locally, by running the following command in the root directory of this repo:</p> <pre><code>docker build -t pysr .\n</code></pre> <p>This builds an image called <code>pysr</code> for your system's architecture, which also contains IPython. You can select a specific version of Python and Julia with:</p> <pre><code>docker build -t pysr --build-arg JLVERSION=1.10.0 --build-arg PYVERSION=3.11.6 .\n</code></pre> <p>You can then run with this dockerfile using:</p> <pre><code>docker run -it --rm -v \"$PWD:/data\" pysr ipython\n</code></pre> <p>which will link the current directory to the container's <code>/data</code> directory and then launch ipython.</p> <p>If you have issues building for your system's architecture, you can emulate another architecture by including <code>--platform linux/amd64</code>, before the <code>build</code> and <code>run</code> commands.</p>"},{"location":"#contributors","title":"Contributors \u2728","text":"<p>We are eager to welcome new contributors! Check out our contributors guide for tips \ud83d\ude80. If you have an idea for a new feature, don't hesitate to share it on the issues or discussions page.</p> <sub>Mark Kittisopikul</sub>\ud83d\udcbb \ud83d\udca1 \ud83d\ude87 \ud83d\udce6 \ud83d\udce3 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f <sub>T Coxon</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udd0c \ud83d\udca1 \ud83d\ude87 \ud83d\udea7 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f \ud83d\udcd3 <sub>Dhananjay Ashok</sub>\ud83d\udcbb \ud83c\udf0d \ud83d\udca1 \ud83d\udea7 \u26a0\ufe0f <sub>Johan Bl\u00e5b\u00e4ck</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \u26a0\ufe0f \ud83d\udcd3 <sub>JuliusMartensen</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83d\udd0c \ud83d\udca1 \ud83d\ude87 \ud83d\udea7 \ud83d\udce6 \ud83d\udce3 \ud83d\udc40 \ud83d\udd27 \ud83d\udcd3 <sub>ngam</sub>\ud83d\udcbb \ud83d\ude87 \ud83d\udce6 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f <sub>Christopher Rowley</sub>\ud83d\udcbb \ud83d\udca1 \ud83d\ude87 \ud83d\udce6 \ud83d\udc40 <sub>Kaze Wong</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\ude87 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udcd3 <sub>Christopher Rackauckas</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udd0c \ud83d\udca1 \ud83d\ude87 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udd27 \u26a0\ufe0f \ud83d\udcd3 <sub>Patrick Kidger</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83d\udd0c \ud83d\udca1 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udd27 \u26a0\ufe0f \ud83d\udcd3 <sub>Okon Samuel</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83d\udea7 \ud83d\udca1 \ud83d\ude87 \ud83d\udc40 \u26a0\ufe0f \ud83d\udcd3 <sub>William Booth-Clibborn</sub>\ud83d\udcbb \ud83c\udf0d \ud83d\udcd6 \ud83d\udcd3 \ud83d\udea7 \ud83d\udc40 \ud83d\udd27 \u26a0\ufe0f <sub>Pablo Lemos</sub>\ud83d\udc1b \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udcd3 <sub>Jerry Ling</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udcd6 \ud83c\udf0d \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udcd3 <sub>Charles Fox</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\udea7 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \ud83d\udcd3 <sub>Johann Brehmer</sub>\ud83d\udcbb \ud83d\udcd6 \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udd2c \u26a0\ufe0f \ud83d\udcd3 <sub>Marius Millea</sub>\ud83d\udcbb \ud83d\udca1 \ud83d\udce3 \ud83d\udc40 \ud83d\udcd3 <sub>Coba</sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udca1 \ud83d\udc40 \ud83d\udcd3 <sub>foxtran</sub>\ud83d\udcbb \ud83d\udca1 \ud83d\udea7 \ud83d\udd27 \ud83d\udcd3 <sub>Shah Mahdi Hasan </sub>\ud83d\udc1b \ud83d\udcbb \ud83d\udc40 \ud83d\udcd3 <sub>Pietro Monticone</sub>\ud83d\udc1b \ud83d\udcd6 \ud83d\udca1 <sub>Mateusz Kubica</sub>\ud83d\udcd6 \ud83d\udca1 <sub>Jay Wadekar</sub>\ud83d\udc1b \ud83d\udca1 \ud83d\udce3 \ud83d\udd2c <sub>Anthony Blaom, PhD</sub>\ud83d\ude87 \ud83d\udca1 \ud83d\udc40 <sub>Jgmedina95</sub>\ud83d\udc1b \ud83d\udca1 \ud83d\udc40 <sub>Michael Abbott</sub>\ud83d\udcbb \ud83d\udca1 \ud83d\udc40 \ud83d\udd27 <sub>Oscar Smith</sub>\ud83d\udcbb \ud83d\udca1 <sub>Eric Hanson</sub>\ud83d\udca1 \ud83d\udce3 \ud83d\udcd3 <sub>Henrique Becker</sub>\ud83d\udcbb \ud83d\udca1 \ud83d\udc40 <sub>qwertyjl</sub>\ud83d\udc1b \ud83d\udcd6 \ud83d\udca1 \ud83d\udcd3 <sub>Rik Huijzer</sub>\ud83d\udca1 \ud83d\ude87 <sub>Hongyu Wang</sub>\ud83d\udca1 \ud83d\udce3 \ud83d\udd2c <sub>Zehao Jin</sub>\ud83d\udd2c \ud83d\udce3 <sub>Tanner Mengel</sub>\ud83d\udd2c \ud83d\udce3 <sub>Arthur Grundner</sub>\ud83d\udd2c \ud83d\udce3 <sub>sjwetzel</sub>\ud83d\udd2c \ud83d\udce3 \ud83d\udcd3 <sub>Saurav Maheshkar</sub>\ud83d\udd27"},{"location":"_api/","title":"PySRRegressor Reference","text":"<p><code>PySRRegressor</code> has many options for controlling a symbolic regression search. Let's look at them below.</p> <p>PARAMSKEY</p>"},{"location":"_api/#pysrregressor-functions","title":"PySRRegressor Functions","text":""},{"location":"_api/#pysr.PySRRegressor.fit","title":"<code>fit(X, y, Xresampled=None, weights=None, variable_names=None, complexity_of_variables=None, X_units=None, y_units=None)</code>","text":"<p>Search for equations to fit the dataset and store them in <code>self.equations_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray | DataFrame</code> <p>Target values of shape (n_samples,) or (n_samples, n_targets). Will be cast to X's dtype if necessary.</p> required <code>Xresampled</code> <code>ndarray | DataFrame</code> <p>Resampled training data, of shape (n_resampled, n_features), to generate a denoised data on. This will be used as the training data, rather than <code>X</code>.</p> <code>None</code> <code>weights</code> <code>ndarray | DataFrame</code> <p>Weight array of the same shape as <code>y</code>. Each element is how to weight the mean-square-error loss for that particular element of <code>y</code>. Alternatively, if a custom <code>loss</code> was set, it will can be used in arbitrary ways.</p> <code>None</code> <code>variable_names</code> <code>list[str]</code> <p>A list of names for the variables, rather than \"x0\", \"x1\", etc. If <code>X</code> is a pandas dataframe, the column names will be used instead of <code>variable_names</code>. Cannot contain spaces or special characters. Avoid variable names which are also function names in <code>sympy</code>, such as \"N\".</p> <code>None</code> <code>X_units</code> <code>list[str]</code> <p>A list of units for each variable in <code>X</code>. Each unit should be a string representing a Julia expression. See DynamicQuantities.jl https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more information.</p> <code>None</code> <code>y_units</code> <code>str | list[str]</code> <p>Similar to <code>X_units</code>, but as a unit for the target variable, <code>y</code>. If <code>y</code> is a matrix, a list of units should be passed. If <code>X_units</code> is given but <code>y_units</code> is not, then <code>y_units</code> will be arbitrary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>pysr/sr.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    Xresampled=None,\n    weights=None,\n    variable_names: Optional[ArrayLike[str]] = None,\n    complexity_of_variables: Optional[\n        Union[int, float, List[Union[int, float]]]\n    ] = None,\n    X_units: Optional[ArrayLike[str]] = None,\n    y_units: Optional[Union[str, ArrayLike[str]]] = None,\n) -&gt; \"PySRRegressor\":\n    \"\"\"\n    Search for equations to fit the dataset and store them in `self.equations_`.\n\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape (n_samples, n_features).\n    y : ndarray | pandas.DataFrame\n        Target values of shape (n_samples,) or (n_samples, n_targets).\n        Will be cast to X's dtype if necessary.\n    Xresampled : ndarray | pandas.DataFrame\n        Resampled training data, of shape (n_resampled, n_features),\n        to generate a denoised data on. This\n        will be used as the training data, rather than `X`.\n    weights : ndarray | pandas.DataFrame\n        Weight array of the same shape as `y`.\n        Each element is how to weight the mean-square-error loss\n        for that particular element of `y`. Alternatively,\n        if a custom `loss` was set, it will can be used\n        in arbitrary ways.\n    variable_names : list[str]\n        A list of names for the variables, rather than \"x0\", \"x1\", etc.\n        If `X` is a pandas dataframe, the column names will be used\n        instead of `variable_names`. Cannot contain spaces or special\n        characters. Avoid variable names which are also\n        function names in `sympy`, such as \"N\".\n    X_units : list[str]\n        A list of units for each variable in `X`. Each unit should be\n        a string representing a Julia expression. See DynamicQuantities.jl\n        https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more\n        information.\n    y_units : str | list[str]\n        Similar to `X_units`, but as a unit for the target variable, `y`.\n        If `y` is a matrix, a list of units should be passed. If `X_units`\n        is given but `y_units` is not, then `y_units` will be arbitrary.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    # Init attributes that are not specified in BaseEstimator\n    if self.warm_start and hasattr(self, \"julia_state_stream_\"):\n        pass\n    else:\n        if hasattr(self, \"julia_state_stream_\"):\n            warnings.warn(\n                \"The discovered expressions are being reset. \"\n                \"Please set `warm_start=True` if you wish to continue \"\n                \"to start a search where you left off.\",\n            )\n\n        self.equations_ = None\n        self.nout_ = 1\n        self.selection_mask_ = None\n        self.julia_state_stream_ = None\n        self.julia_options_stream_ = None\n        self.complexity_of_variables_ = None\n        self.X_units_ = None\n        self.y_units_ = None\n\n    self._setup_equation_file()\n\n    runtime_params = self._validate_and_modify_params()\n\n    (\n        X,\n        y,\n        Xresampled,\n        weights,\n        variable_names,\n        complexity_of_variables,\n        X_units,\n        y_units,\n    ) = self._validate_and_set_fit_params(\n        X,\n        y,\n        Xresampled,\n        weights,\n        variable_names,\n        complexity_of_variables,\n        X_units,\n        y_units,\n    )\n\n    if X.shape[0] &gt; 10000 and not self.batching:\n        warnings.warn(\n            \"Note: you are running with more than 10,000 datapoints. \"\n            \"You should consider turning on batching (https://astroautomata.com/PySR/options/#batching). \"\n            \"You should also reconsider if you need that many datapoints. \"\n            \"Unless you have a large amount of noise (in which case you \"\n            \"should smooth your dataset first), generally &lt; 10,000 datapoints \"\n            \"is enough to find a functional form with symbolic regression. \"\n            \"More datapoints will lower the search speed.\"\n        )\n\n    random_state = check_random_state(self.random_state)  # For np random\n    seed = cast(int, random_state.randint(0, 2**31 - 1))  # For julia random\n\n    # Pre transformations (feature selection and denoising)\n    X, y, variable_names, complexity_of_variables, X_units, y_units = (\n        self._pre_transform_training_data(\n            X,\n            y,\n            Xresampled,\n            variable_names,\n            complexity_of_variables,\n            X_units,\n            y_units,\n            random_state,\n        )\n    )\n\n    # Warn about large feature counts (still warn if feature count is large\n    # after running feature selection)\n    if self.n_features_in_ &gt;= 10:\n        warnings.warn(\n            \"Note: you are running with 10 features or more. \"\n            \"Genetic algorithms like used in PySR scale poorly with large numbers of features. \"\n            \"You should run PySR for more `niterations` to ensure it can find \"\n            \"the correct variables, and consider using a larger `maxsize`.\"\n        )\n\n    # Assertion checks\n    use_custom_variable_names = variable_names is not None\n    # TODO: this is always true.\n\n    _check_assertions(\n        X,\n        use_custom_variable_names,\n        variable_names,\n        complexity_of_variables,\n        weights,\n        y,\n        X_units,\n        y_units,\n    )\n\n    # Initially, just save model parameters, so that\n    # it can be loaded from an early exit:\n    if not self.temp_equation_file:\n        self._checkpoint()\n\n    # Perform the search:\n    self._run(X, y, runtime_params, weights=weights, seed=seed)\n\n    # Then, after fit, we save again, so the pickle file contains\n    # the equations:\n    if not self.temp_equation_file:\n        self._checkpoint()\n\n    return self\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.predict","title":"<code>predict(X, index=None)</code>","text":"<p>Predict y from input X using the equation chosen by <code>model_selection</code>.</p> <p>You may see what equation is used by printing this object. X should have the same columns as the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape <code>(n_samples, n_features)</code>.</p> required <code>index</code> <code>int | list[int]</code> <p>If you want to compute the output of an expression using a particular row of <code>self.equations_</code>, you may specify the index here. For multiple output equations, you must pass a list of indices in the same order.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_predicted</code> <code>ndarray of shape (n_samples, nout_)</code> <p>Values predicted by substituting <code>X</code> into the fitted symbolic regression model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises if the <code>best_equation</code> cannot be evaluated.</p> Source code in <code>pysr/sr.py</code> <pre><code>def predict(self, X, index=None):\n    \"\"\"\n    Predict y from input X using the equation chosen by `model_selection`.\n\n    You may see what equation is used by printing this object. X should\n    have the same columns as the training data.\n\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape `(n_samples, n_features)`.\n    index : int | list[int]\n        If you want to compute the output of an expression using a\n        particular row of `self.equations_`, you may specify the index here.\n        For multiple output equations, you must pass a list of indices\n        in the same order.\n\n    Returns\n    -------\n    y_predicted : ndarray of shape (n_samples, nout_)\n        Values predicted by substituting `X` into the fitted symbolic\n        regression model.\n\n    Raises\n    ------\n    ValueError\n        Raises if the `best_equation` cannot be evaluated.\n    \"\"\"\n    check_is_fitted(\n        self, attributes=[\"selection_mask_\", \"feature_names_in_\", \"nout_\"]\n    )\n    best_equation = self.get_best(index=index)\n\n    # When X is an numpy array or a pandas dataframe with a RangeIndex,\n    # the self.feature_names_in_ generated during fit, for the same X,\n    # will cause a warning to be thrown during _validate_data.\n    # To avoid this, convert X to a dataframe, apply the selection mask,\n    # and then set the column/feature_names of X to be equal to those\n    # generated during fit.\n    if not isinstance(X, pd.DataFrame):\n        X = check_array(X)\n        X = pd.DataFrame(X)\n    if isinstance(X.columns, pd.RangeIndex):\n        if self.selection_mask_ is not None:\n            # RangeIndex enforces column order allowing columns to\n            # be correctly filtered with self.selection_mask_\n            X = X[X.columns[self.selection_mask_]]\n        X.columns = self.feature_names_in_\n    # Without feature information, CallableEquation/lambda_format equations\n    # require that the column order of X matches that of the X used during\n    # the fitting process. _validate_data removes this feature information\n    # when it converts the dataframe to an np array. Thus, to ensure feature\n    # order is preserved after conversion, the dataframe columns must be\n    # reordered/reindexed to match those of the transformed (denoised and\n    # feature selected) X in fit.\n    X = X.reindex(columns=self.feature_names_in_)\n    X = self._validate_data_X(X)\n\n    try:\n        if isinstance(best_equation, list):\n            assert self.nout_ &gt; 1\n            return np.stack(\n                [eq[\"lambda_format\"](X) for eq in best_equation], axis=1\n            )\n        else:\n            return best_equation[\"lambda_format\"](X)\n    except Exception as error:\n        raise ValueError(\n            \"Failed to evaluate the expression. \"\n            \"If you are using a custom operator, make sure to define it in `extra_sympy_mappings`, \"\n            \"e.g., `model.set_params(extra_sympy_mappings={'inv': lambda x: 1/x})`, where \"\n            \"`lambda x: 1/x` is a valid SymPy function defining the operator. \"\n            \"You can then run `model.refresh()` to re-load the expressions.\"\n        ) from error\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.from_file","title":"<code>from_file(equation_file, *, binary_operators=None, unary_operators=None, n_features_in=None, feature_names_in=None, selection_mask=None, nout=1, **pysr_kwargs)</code>  <code>classmethod</code>","text":"<p>Create a model from a saved model checkpoint or equation file.</p> <p>Parameters:</p> Name Type Description Default <code>equation_file</code> <code>str or Path</code> <p>Path to a pickle file containing a saved model, or a csv file containing equations.</p> required <code>binary_operators</code> <code>list[str]</code> <p>The same binary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>unary_operators</code> <code>list[str]</code> <p>The same unary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>n_features_in</code> <code>int</code> <p>Number of features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>feature_names_in</code> <code>list[str]</code> <p>Names of the features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>selection_mask</code> <code>NDArray[bool_]</code> <p>If using <code>select_k_features</code>, you must pass <code>model.selection_mask_</code> here. Not needed if loading from a pickle file.</p> <code>None</code> <code>nout</code> <code>int</code> <p>Number of outputs of the model. Not needed if loading from a pickle file. Default is <code>1</code>.</p> <code>1</code> <code>**pysr_kwargs</code> <code>dict</code> <p>Any other keyword arguments to initialize the PySRRegressor object. These will overwrite those stored in the pickle file. Not needed if loading from a pickle file.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>model</code> <code>PySRRegressor</code> <p>The model with fitted equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    equation_file: PathLike,\n    *,\n    binary_operators: Optional[List[str]] = None,\n    unary_operators: Optional[List[str]] = None,\n    n_features_in: Optional[int] = None,\n    feature_names_in: Optional[ArrayLike[str]] = None,\n    selection_mask: Optional[NDArray[np.bool_]] = None,\n    nout: int = 1,\n    **pysr_kwargs,\n):\n    \"\"\"\n    Create a model from a saved model checkpoint or equation file.\n\n    Parameters\n    ----------\n    equation_file : str or Path\n        Path to a pickle file containing a saved model, or a csv file\n        containing equations.\n    binary_operators : list[str]\n        The same binary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    unary_operators : list[str]\n        The same unary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    n_features_in : int\n        Number of features passed to the model.\n        Not needed if loading from a pickle file.\n    feature_names_in : list[str]\n        Names of the features passed to the model.\n        Not needed if loading from a pickle file.\n    selection_mask : NDArray[np.bool_]\n        If using `select_k_features`, you must pass `model.selection_mask_` here.\n        Not needed if loading from a pickle file.\n    nout : int\n        Number of outputs of the model.\n        Not needed if loading from a pickle file.\n        Default is `1`.\n    **pysr_kwargs : dict\n        Any other keyword arguments to initialize the PySRRegressor object.\n        These will overwrite those stored in the pickle file.\n        Not needed if loading from a pickle file.\n\n    Returns\n    -------\n    model : PySRRegressor\n        The model with fitted equations.\n    \"\"\"\n\n    pkl_filename = _csv_filename_to_pkl_filename(equation_file)\n\n    # Try to load model from &lt;equation_file&gt;.pkl\n    print(f\"Checking if {pkl_filename} exists...\")\n    if os.path.exists(pkl_filename):\n        print(f\"Loading model from {pkl_filename}\")\n        assert binary_operators is None\n        assert unary_operators is None\n        assert n_features_in is None\n        with open(pkl_filename, \"rb\") as f:\n            model = pkl.load(f)\n        # Change equation_file_ to be in the same dir as the pickle file\n        base_dir = os.path.dirname(pkl_filename)\n        base_equation_file = os.path.basename(model.equation_file_)\n        model.equation_file_ = os.path.join(base_dir, base_equation_file)\n\n        # Update any parameters if necessary, such as\n        # extra_sympy_mappings:\n        model.set_params(**pysr_kwargs)\n        if \"equations_\" not in model.__dict__ or model.equations_ is None:\n            model.refresh()\n\n        return model\n\n    # Else, we re-create it.\n    print(\n        f\"{pkl_filename} does not exist, \"\n        \"so we must create the model from scratch.\"\n    )\n    assert binary_operators is not None or unary_operators is not None\n    assert n_features_in is not None\n\n    # TODO: copy .bkup file if exists.\n    model = cls(\n        equation_file=str(equation_file),\n        binary_operators=binary_operators,\n        unary_operators=unary_operators,\n        **pysr_kwargs,\n    )\n\n    model.nout_ = nout\n    model.n_features_in_ = n_features_in\n\n    if feature_names_in is None:\n        model.feature_names_in_ = np.array([f\"x{i}\" for i in range(n_features_in)])\n        model.display_feature_names_in_ = np.array(\n            [f\"x{_subscriptify(i)}\" for i in range(n_features_in)]\n        )\n    else:\n        assert len(feature_names_in) == n_features_in\n        model.feature_names_in_ = feature_names_in\n        model.display_feature_names_in_ = feature_names_in\n\n    if selection_mask is None:\n        model.selection_mask_ = np.ones(n_features_in, dtype=np.bool_)\n    else:\n        model.selection_mask_ = selection_mask\n\n    model.refresh(checkpoint_file=equation_file)\n\n    return model\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.sympy","title":"<code>sympy(index=None)</code>","text":"<p>Return sympy representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str, list[str] of length nout_</code> <p>SymPy representation of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def sympy(self, index=None):\n    \"\"\"\n    Return sympy representation of the equation(s) chosen by `model_selection`.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n\n    Returns\n    -------\n    best_equation : str, list[str] of length nout_\n        SymPy representation of the best equation.\n    \"\"\"\n    self.refresh()\n    best_equation = self.get_best(index=index)\n    if isinstance(best_equation, list):\n        assert self.nout_ &gt; 1\n        return [eq[\"sympy_format\"] for eq in best_equation]\n    else:\n        return best_equation[\"sympy_format\"]\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.latex","title":"<code>latex(index=None, precision=3)</code>","text":"<p>Return latex representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representation. Default is <code>3</code>.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str or list[str] of length nout_</code> <p>LaTeX expression of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex(self, index=None, precision=3):\n    \"\"\"\n    Return latex representation of the equation(s) chosen by `model_selection`.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representation.\n        Default is `3`.\n\n    Returns\n    -------\n    best_equation : str or list[str] of length nout_\n        LaTeX expression of the best equation.\n    \"\"\"\n    self.refresh()\n    sympy_representation = self.sympy(index=index)\n    if self.nout_ &gt; 1:\n        output = []\n        for s in sympy_representation:\n            latex = sympy2latex(s, prec=precision)\n            output.append(latex)\n        return output\n    return sympy2latex(sympy_representation, prec=precision)\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.pytorch","title":"<code>pytorch(index=None)</code>","text":"<p>Return pytorch representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a PyTorch module containing the parameters as trainable attributes. You can use the module like any other PyTorch module: <code>module(X)</code>, where <code>X</code> is a tensor with the same column ordering as trained with.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>Module</code> <p>PyTorch module representing the expression.</p> Source code in <code>pysr/sr.py</code> <pre><code>def pytorch(self, index=None):\n    \"\"\"\n    Return pytorch representation of the equation(s) chosen by `model_selection`.\n\n    Each equation (multiple given if there are multiple outputs) is a PyTorch module\n    containing the parameters as trainable attributes. You can use the module like\n    any other PyTorch module: `module(X)`, where `X` is a tensor with the same\n    column ordering as trained with.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n\n    Returns\n    -------\n    best_equation : torch.nn.Module\n        PyTorch module representing the expression.\n    \"\"\"\n    self.set_params(output_torch_format=True)\n    self.refresh()\n    best_equation = self.get_best(index=index)\n    if isinstance(best_equation, list):\n        return [eq[\"torch_format\"] for eq in best_equation]\n    else:\n        return best_equation[\"torch_format\"]\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.jax","title":"<code>jax(index=None)</code>","text":"<p>Return jax representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a dictionary containing {\"callable\": func, \"parameters\": params}. To call <code>func</code>, pass func(X, params). This function is differentiable using <code>jax.grad</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>dict[str, Any]</code> <p>Dictionary of callable jax function in \"callable\" key, and jax array of parameters as \"parameters\" key.</p> Source code in <code>pysr/sr.py</code> <pre><code>def jax(self, index=None):\n    \"\"\"\n    Return jax representation of the equation(s) chosen by `model_selection`.\n\n    Each equation (multiple given if there are multiple outputs) is a dictionary\n    containing {\"callable\": func, \"parameters\": params}. To call `func`, pass\n    func(X, params). This function is differentiable using `jax.grad`.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n\n    Returns\n    -------\n    best_equation : dict[str, Any]\n        Dictionary of callable jax function in \"callable\" key,\n        and jax array of parameters as \"parameters\" key.\n    \"\"\"\n    self.set_params(output_jax_format=True)\n    self.refresh()\n    best_equation = self.get_best(index=index)\n    if isinstance(best_equation, list):\n        assert self.nout_ &gt; 1\n        return [eq[\"jax_format\"] for eq in best_equation]\n    else:\n        return best_equation[\"jax_format\"]\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.latex_table","title":"<code>latex_table(indices=None, precision=3, columns=['equation', 'complexity', 'loss', 'score'])</code>","text":"<p>Create a LaTeX/booktabs table for all, or some, of the equations.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int] | list[list[int]]</code> <p>If you wish to select a particular subset of equations from <code>self.equations_</code>, give the row numbers here. By default, all equations will be used. If there are multiple output features, then pass a list of lists.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representations. Default is <code>3</code>.</p> <code>3</code> <code>columns</code> <code>list[str]</code> <p>Which columns to include in the table. Default is <code>[\"equation\", \"complexity\", \"loss\", \"score\"]</code>.</p> <code>['equation', 'complexity', 'loss', 'score']</code> <p>Returns:</p> Name Type Description <code>latex_table_str</code> <code>str</code> <p>A string that will render a table in LaTeX of the equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex_table(\n    self,\n    indices=None,\n    precision=3,\n    columns=[\"equation\", \"complexity\", \"loss\", \"score\"],\n):\n    \"\"\"Create a LaTeX/booktabs table for all, or some, of the equations.\n\n    Parameters\n    ----------\n    indices : list[int] | list[list[int]]\n        If you wish to select a particular subset of equations from\n        `self.equations_`, give the row numbers here. By default,\n        all equations will be used. If there are multiple output\n        features, then pass a list of lists.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representations.\n        Default is `3`.\n    columns : list[str]\n        Which columns to include in the table.\n        Default is `[\"equation\", \"complexity\", \"loss\", \"score\"]`.\n\n    Returns\n    -------\n    latex_table_str : str\n        A string that will render a table in LaTeX of the equations.\n    \"\"\"\n    self.refresh()\n\n    if isinstance(self.equations_, list):\n        if indices is not None:\n            assert isinstance(indices, list)\n            assert isinstance(indices[0], list)\n            assert len(indices) == self.nout_\n\n        table_string = sympy2multilatextable(\n            self.equations_, indices=indices, precision=precision, columns=columns\n        )\n    elif isinstance(self.equations_, pd.DataFrame):\n        if indices is not None:\n            assert isinstance(indices, list)\n            assert isinstance(indices[0], int)\n\n        table_string = sympy2latextable(\n            self.equations_, indices=indices, precision=precision, columns=columns\n        )\n    else:\n        raise ValueError(\n            \"Invalid type for equations_ to pass to `latex_table`. \"\n            \"Expected a DataFrame or a list of DataFrames.\"\n        )\n\n    return with_preamble(table_string)\n</code></pre>"},{"location":"_api/#pysr.PySRRegressor.refresh","title":"<code>refresh(checkpoint_file=None)</code>","text":"<p>Update self.equations_ with any new options passed.</p> <p>For example, updating <code>extra_sympy_mappings</code> will require a <code>.refresh()</code> to update the equations.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_file</code> <code>str or Path</code> <p>Path to checkpoint hall of fame file to be loaded. The default will use the set <code>equation_file_</code>.</p> <code>None</code> Source code in <code>pysr/sr.py</code> <pre><code>def refresh(self, checkpoint_file: Optional[PathLike] = None) -&gt; None:\n    \"\"\"\n    Update self.equations_ with any new options passed.\n\n    For example, updating `extra_sympy_mappings`\n    will require a `.refresh()` to update the equations.\n\n    Parameters\n    ----------\n    checkpoint_file : str or Path\n        Path to checkpoint hall of fame file to be loaded.\n        The default will use the set `equation_file_`.\n    \"\"\"\n    if checkpoint_file is not None:\n        self.equation_file_ = checkpoint_file\n        self.equation_file_contents_ = None\n    check_is_fitted(self, attributes=[\"equation_file_\"])\n    self.equations_ = self.get_hof()\n</code></pre>"},{"location":"api-advanced/","title":"Internal Reference","text":""},{"location":"api-advanced/#julia-interface","title":"Julia Interface","text":"<p>Functions for initializing the Julia environment and installing deps.</p>"},{"location":"api-advanced/#pysr.julia_helpers.init_julia","title":"<code>init_julia(*args, **kwargs)</code>","text":"Source code in <code>pysr/deprecated.py</code> <pre><code>def init_julia(*args, **kwargs):\n    del args, kwargs\n    warnings.warn(\n        \"The `init_julia` function has been removed. \"\n        \"Julia is now initialized automatically at import time.\",\n        FutureWarning,\n    )\n    return jl\n</code></pre>"},{"location":"api-advanced/#pysr.julia_helpers.install","title":"<code>install(*args, **kwargs)</code>","text":"Source code in <code>pysr/deprecated.py</code> <pre><code>def install(*args, **kwargs):\n    del args, kwargs\n    warnings.warn(\n        \"The `install` function has been removed. \"\n        \"PySR now uses the `juliacall` package to install its dependencies automatically at import time. \",\n        FutureWarning,\n    )\n</code></pre>"},{"location":"api-advanced/#exporting-to-latex","title":"Exporting to LaTeX","text":"<p>Functions to help export PySR equations to LaTeX.</p>"},{"location":"api-advanced/#pysr.export_latex.generate_table_environment","title":"<code>generate_table_environment(columns=['equation', 'complexity', 'loss'])</code>","text":"Source code in <code>pysr/export_latex.py</code> <pre><code>def generate_table_environment(\n    columns: List[str] = [\"equation\", \"complexity\", \"loss\"]\n) -&gt; Tuple[str, str]:\n    margins = \"c\" * len(columns)\n    column_map = {\n        \"complexity\": \"Complexity\",\n        \"loss\": \"Loss\",\n        \"equation\": \"Equation\",\n        \"score\": \"Score\",\n    }\n    columns = [column_map[col] for col in columns]\n    top_pieces = [\n        r\"\\begin{table}[h]\",\n        r\"\\begin{center}\",\n        r\"\\begin{tabular}{@{}\" + margins + r\"@{}}\",\n        r\"\\toprule\",\n        \" &amp; \".join(columns) + r\" \\\\\",\n        r\"\\midrule\",\n    ]\n\n    bottom_pieces = [\n        r\"\\bottomrule\",\n        r\"\\end{tabular}\",\n        r\"\\end{center}\",\n        r\"\\end{table}\",\n    ]\n    top_latex_table = \"\\n\".join(top_pieces)\n    bottom_latex_table = \"\\n\".join(bottom_pieces)\n\n    return top_latex_table, bottom_latex_table\n</code></pre>"},{"location":"api-advanced/#exporting-to-jax","title":"Exporting to JAX","text":""},{"location":"api-advanced/#pysr.export_jax.sympy2jax","title":"<code>sympy2jax(expression, symbols_in, selection=None, extra_jax_mappings=None)</code>","text":"<p>Returns a function f and its parameters; the function takes an input matrix, and a list of arguments:         f(X, parameters) where the parameters appear in the JAX equation.</p>"},{"location":"api-advanced/#pysr.export_jax.sympy2jax--examples","title":"Examples:","text":"<pre><code>Let's create a function in SymPy:\n```python\nx, y = symbols('x y')\ncosx = 1.0 * sympy.cos(x) + 3.2 * y\n```\nLet's get the JAX version. We pass the equation, and\nthe symbols required.\n```python\nf, params = sympy2jax(cosx, [x, y])\n```\nThe order you supply the symbols is the same order\nyou should supply the features when calling\nthe function `f` (shape `[nrows, nfeatures]`).\nIn this case, features=2 for x and y.\nThe `params` in this case will be\n`jnp.array([1.0, 3.2])`. You pass these parameters\nwhen calling the function, which will let you change them\nand take gradients.\n\nLet's generate some JAX data to pass:\n```python\nkey = random.PRNGKey(0)\nX = random.normal(key, (10, 2))\n```\n\nWe can call the function with:\n```python\nf(X, params)\n\n#&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n#                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n#                3.5427954 , -2.7479894 ], dtype=float32)\n```\n\nWe can take gradients with respect\nto the parameters for each row with JAX\ngradient parameters now:\n```python\njac_f = jax.jacobian(f, argnums=1)\njac_f(X, params)\n\n#&gt; DeviceArray([[ 0.49364874, -0.9692889 ],\n#               [ 0.8283714 , -0.0318858 ],\n#               [-0.7447336 , -1.8784496 ],\n#               [ 0.70755106, -0.3137085 ],\n#               [ 0.944834  ,  1.767703  ],\n#               [ 0.51673377,  1.4111717 ],\n#               [ 0.87347716, -0.52637756],\n#               [ 0.8760679 ,  1.0549792 ],\n#               [ 0.9961824 ,  0.79581654],\n#               [-0.88465923, -0.5822907 ]], dtype=float32)\n```\n\nWe can also JIT-compile our function:\n```python\ncompiled_f = jax.jit(f)\ncompiled_f(X, params)\n\n#&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n#                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n#                3.5427954 , -2.7479894 ], dtype=float32)\n```\n</code></pre> Source code in <code>pysr/export_jax.py</code> <pre><code>def sympy2jax(expression, symbols_in, selection=None, extra_jax_mappings=None):\n    \"\"\"Returns a function f and its parameters;\n    the function takes an input matrix, and a list of arguments:\n            f(X, parameters)\n    where the parameters appear in the JAX equation.\n\n    # Examples:\n\n        Let's create a function in SymPy:\n        ```python\n        x, y = symbols('x y')\n        cosx = 1.0 * sympy.cos(x) + 3.2 * y\n        ```\n        Let's get the JAX version. We pass the equation, and\n        the symbols required.\n        ```python\n        f, params = sympy2jax(cosx, [x, y])\n        ```\n        The order you supply the symbols is the same order\n        you should supply the features when calling\n        the function `f` (shape `[nrows, nfeatures]`).\n        In this case, features=2 for x and y.\n        The `params` in this case will be\n        `jnp.array([1.0, 3.2])`. You pass these parameters\n        when calling the function, which will let you change them\n        and take gradients.\n\n        Let's generate some JAX data to pass:\n        ```python\n        key = random.PRNGKey(0)\n        X = random.normal(key, (10, 2))\n        ```\n\n        We can call the function with:\n        ```python\n        f(X, params)\n\n        #&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n        #                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n        #                3.5427954 , -2.7479894 ], dtype=float32)\n        ```\n\n        We can take gradients with respect\n        to the parameters for each row with JAX\n        gradient parameters now:\n        ```python\n        jac_f = jax.jacobian(f, argnums=1)\n        jac_f(X, params)\n\n        #&gt; DeviceArray([[ 0.49364874, -0.9692889 ],\n        #               [ 0.8283714 , -0.0318858 ],\n        #               [-0.7447336 , -1.8784496 ],\n        #               [ 0.70755106, -0.3137085 ],\n        #               [ 0.944834  ,  1.767703  ],\n        #               [ 0.51673377,  1.4111717 ],\n        #               [ 0.87347716, -0.52637756],\n        #               [ 0.8760679 ,  1.0549792 ],\n        #               [ 0.9961824 ,  0.79581654],\n        #               [-0.88465923, -0.5822907 ]], dtype=float32)\n        ```\n\n        We can also JIT-compile our function:\n        ```python\n        compiled_f = jax.jit(f)\n        compiled_f(X, params)\n\n        #&gt; DeviceArray([-2.6080756 ,  0.72633684, -6.7557726 , -0.2963162 ,\n        #                6.6014843 ,  5.032483  , -0.810931  ,  4.2520013 ,\n        #                3.5427954 , -2.7479894 ], dtype=float32)\n        ```\n    \"\"\"\n    _initialize_jax()\n    global jax_initialized\n    global jax\n    global jnp\n    global jsp\n\n    parameters = []\n    functional_form_text = sympy2jaxtext(\n        expression, parameters, symbols_in, extra_jax_mappings\n    )\n    hash_string = \"A_\" + str(abs(hash(str(expression) + str(symbols_in))))\n    text = f\"def {hash_string}(X, parameters):\\n\"\n    if selection is not None:\n        # Impose the feature selection:\n        text += f\"    X = X[:, {list(selection)}]\\n\"\n    text += \"    return \"\n    text += functional_form_text\n    ldict = {}\n    exec(text, globals(), ldict)\n    return ldict[hash_string], jnp.array(parameters)\n</code></pre>"},{"location":"api-advanced/#pysr.export_jax.sympy2jaxtext","title":"<code>sympy2jaxtext(expr, parameters, symbols_in, extra_jax_mappings=None)</code>","text":"Source code in <code>pysr/export_jax.py</code> <pre><code>def sympy2jaxtext(expr, parameters, symbols_in, extra_jax_mappings=None):\n    if issubclass(expr.func, sympy.Float):\n        parameters.append(float(expr))\n        return f\"parameters[{len(parameters) - 1}]\"\n    elif issubclass(expr.func, sympy.Rational) or issubclass(\n        expr.func, sympy.NumberSymbol\n    ):\n        return f\"{float(expr)}\"\n    elif issubclass(expr.func, sympy.Integer):\n        return f\"{int(expr)}\"\n    elif issubclass(expr.func, sympy.Symbol):\n        return (\n            f\"X[:, {[i for i in range(len(symbols_in)) if symbols_in[i] == expr][0]}]\"\n        )\n    if extra_jax_mappings is None:\n        extra_jax_mappings = {}\n    try:\n        _func = {**_jnp_func_lookup, **extra_jax_mappings}[expr.func]\n    except KeyError:\n        raise KeyError(\n            f\"Function {expr.func} was not found in JAX function mappings.\"\n            \"Please add it to extra_jax_mappings in the format, e.g., \"\n            \"{sympy.sqrt: 'jnp.sqrt'}.\"\n        )\n    args = [\n        sympy2jaxtext(\n            arg, parameters, symbols_in, extra_jax_mappings=extra_jax_mappings\n        )\n        for arg in expr.args\n    ]\n    if _func == MUL:\n        return \" * \".join([\"(\" + arg + \")\" for arg in args])\n    if _func == ADD:\n        return \" + \".join([\"(\" + arg + \")\" for arg in args])\n    return f'{_func}({\", \".join(args)})'\n</code></pre>"},{"location":"api-advanced/#exporting-to-pytorch","title":"Exporting to PyTorch","text":""},{"location":"api-advanced/#pysr.export_torch.sympy2torch","title":"<code>sympy2torch(expression, symbols_in, selection=None, extra_torch_mappings=None)</code>","text":"<p>Returns a module for a given sympy expression with trainable parameters;</p> <p>This function will assume the input to the module is a matrix X, where     each column corresponds to each symbol you pass in <code>symbols_in</code>.</p> Source code in <code>pysr/export_torch.py</code> <pre><code>def sympy2torch(expression, symbols_in, selection=None, extra_torch_mappings=None):\n    \"\"\"Returns a module for a given sympy expression with trainable parameters;\n\n    This function will assume the input to the module is a matrix X, where\n        each column corresponds to each symbol you pass in `symbols_in`.\n    \"\"\"\n    global SingleSymPyModule\n\n    _initialize_torch()\n\n    return SingleSymPyModule(\n        expression, symbols_in, selection=selection, extra_funcs=extra_torch_mappings\n    )\n</code></pre>"},{"location":"api/","title":"PySRRegressor Reference","text":"<p><code>PySRRegressor</code> has many options for controlling a symbolic regression search. Let's look at them below.</p>"},{"location":"api/#pysrregressor-parameters","title":"PySRRegressor Parameters","text":""},{"location":"api/#the-algorithm","title":"The Algorithm","text":""},{"location":"api/#creating-the-search-space","title":"Creating the Search Space","text":"<ul> <li> <p><code>binary_operators</code></p> <p>List of strings for binary operators used in the search. See the operators page for more details.</p> <p>Default: <code>[\"+\", \"-\", \"*\", \"/\"]</code></p> </li> <li> <p><code>unary_operators</code></p> <p>Operators which only take a single scalar as input. For example, <code>\"cos\"</code> or <code>\"exp\"</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>maxsize</code></p> <p>Max complexity of an equation.  </p> <p>Default: <code>20</code></p> </li> <li> <p><code>maxdepth</code></p> <p>Max depth of an equation. You can use both <code>maxsize</code> and <code>maxdepth</code>. <code>maxdepth</code> is by default not used.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#setting-the-search-size","title":"Setting the Search Size","text":"<ul> <li> <p><code>niterations</code></p> <p>Number of iterations of the algorithm to run. The best equations are printed and migrate between populations at the end of each iteration.</p> <p>Default: <code>40</code></p> </li> <li> <p><code>populations</code></p> <p>Number of populations running.</p> <p>Default: <code>15</code></p> </li> <li> <p><code>population_size</code></p> <p>Number of individuals in each population.</p> <p>Default: <code>33</code></p> </li> <li> <p><code>ncycles_per_iteration</code></p> <p>Number of total mutations to run, per 10 samples of the population, per iteration.</p> <p>Default: <code>550</code></p> </li> </ul>"},{"location":"api/#the-objective","title":"The Objective","text":"<ul> <li> <p><code>elementwise_loss</code></p> <p>String of Julia code specifying an elementwise loss function. Can either be a loss from LossFunctions.jl, or your own loss written as a function. Examples of custom written losses include: <code>myloss(x, y) = abs(x-y)</code> for non-weighted, or <code>myloss(x, y, w) = w*abs(x-y)</code> for weighted. The included losses include: Regression: <code>LPDistLoss{P}()</code>, <code>L1DistLoss()</code>, <code>L2DistLoss()</code> (mean square), <code>LogitDistLoss()</code>, <code>HuberLoss(d)</code>, <code>L1EpsilonInsLoss(\u03f5)</code>, <code>L2EpsilonInsLoss(\u03f5)</code>, <code>PeriodicLoss(c)</code>, <code>QuantileLoss(\u03c4)</code>. Classification: <code>ZeroOneLoss()</code>, <code>PerceptronLoss()</code>, <code>L1HingeLoss()</code>, <code>SmoothedL1HingeLoss(\u03b3)</code>, <code>ModifiedHuberLoss()</code>, <code>L2MarginLoss()</code>, <code>ExpLoss()</code>, <code>SigmoidLoss()</code>, <code>DWDMarginLoss(q)</code>.</p> <p>Default: <code>\"L2DistLoss()\"</code></p> </li> <li> <p><code>loss_function</code></p> <p>Alternatively, you can specify the full objective function as a snippet of Julia code, including any sort of custom evaluation (including symbolic manipulations beforehand), and any sort of loss function or regularizations. The default <code>loss_function</code> used in SymbolicRegression.jl is roughly equal to: <pre><code>function eval_loss(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n    prediction, flag = eval_tree_array(tree, dataset.X, options)\n    if !flag\n        return L(Inf)\n    end\n    return sum((prediction .- dataset.y) .^ 2) / dataset.n\nend\n</code></pre> where the example elementwise loss is mean-squared error. You may pass a function with the same arguments as this (note that the name of the function doesn't matter). Here, both <code>prediction</code> and <code>dataset.y</code> are 1D arrays of length <code>dataset.n</code>. If using <code>batching</code>, then you should add an <code>idx</code> argument to the function, which is <code>nothing</code> for non-batched, and a 1D array of indices for batched.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>model_selection</code></p> <p>Model selection criterion when selecting a final expression from the list of best expression at each complexity. Can be <code>'accuracy'</code>, <code>'best'</code>, or <code>'score'</code>.  <code>'accuracy'</code> selects the candidate model with the lowest loss (highest accuracy). <code>'score'</code> selects the candidate model with the highest score. Score is defined as the negated derivative of the log-loss with respect to complexity - if an expression has a much better loss at a slightly higher complexity, it is preferred. <code>'best'</code> selects the candidate model with the highest score among expressions with a loss better than at least 1.5x the most accurate model.</p> <p>Default: <code>'best'</code></p> </li> <li> <p><code>dimensional_constraint_penalty</code></p> <p>Additive penalty for if dimensional analysis of an expression fails. By default, this is <code>1000.0</code>.</p> </li> <li> <p><code>dimensionless_constants_only</code></p> <p>Whether to only search for dimensionless constants, if using units.</p> <p>Default: <code>False</code></p> </li> </ul>"},{"location":"api/#working-with-complexities","title":"Working with Complexities","text":"<ul> <li> <p><code>parsimony</code></p> <p>Multiplicative factor for how much to punish complexity.</p> <p>Default: <code>0.0032</code></p> </li> <li> <p><code>constraints</code></p> <p>Dictionary of int (unary) or 2-tuples (binary), this enforces maxsize constraints on the individual arguments of operators. E.g., <code>'pow': (-1, 1)</code> says that power laws can have any complexity left argument, but only 1 complexity in the right argument. Use this to force more interpretable solutions.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>nested_constraints</code></p> <p>Specifies how many times a combination of operators can be nested. For example, <code>{\"sin\": {\"cos\": 0}}, \"cos\": {\"cos\": 2}}</code> specifies that <code>cos</code> may never appear within a <code>sin</code>, but <code>sin</code> can be nested with itself an unlimited number of times. The second term specifies that <code>cos</code> can be nested up to 2 times within a <code>cos</code>, so that <code>cos(cos(cos(x)))</code> is allowed (as well as any combination of <code>+</code> or <code>-</code> within it), but <code>cos(cos(cos(cos(x))))</code> is not allowed. When an operator is not specified, it is assumed that it can be nested an unlimited number of times. This requires that there is no operator which is used both in the unary operators and the binary operators (e.g., <code>-</code> could be both subtract, and negation). For binary operators, you only need to provide a single number: both arguments are treated the same way, and the max of each argument is constrained.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>complexity_of_operators</code></p> <p>If you would like to use a complexity other than 1 for an operator, specify the complexity here. For example, <code>{\"sin\": 2, \"+\": 1}</code> would give a complexity of 2 for each use of the <code>sin</code> operator, and a complexity of 1 for each use of the <code>+</code> operator (which is the default). You may specify real numbers for a complexity, and the total complexity of a tree will be rounded to the nearest integer after computing.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>complexity_of_constants</code></p> <p>Complexity of constants. </p> <p>Default: <code>1</code></p> </li> <li> <p><code>complexity_of_variables</code></p> <p>Global complexity of variables. To set different complexities for different variables, pass a list of complexities to the <code>fit</code> method with keyword <code>complexity_of_variables</code>. You cannot use both.</p> <p>Default: <code>1</code></p> </li> <li> <p><code>warmup_maxsize_by</code></p> <p>Whether to slowly increase max size from a small number up to the maxsize (if greater than 0).  If greater than 0, says the fraction of training time at which the current maxsize will reach the user-passed maxsize.</p> <p>Default: <code>0.0</code></p> </li> <li> <p><code>use_frequency</code></p> <p>Whether to measure the frequency of complexities, and use that instead of parsimony to explore equation space. Will naturally find equations of all complexities.</p> <p>Default: <code>True</code></p> </li> <li> <p><code>use_frequency_in_tournament</code></p> <p>Whether to use the frequency mentioned above in the tournament, rather than just the simulated annealing.</p> <p>Default: <code>True</code></p> </li> <li> <p><code>adaptive_parsimony_scaling</code></p> <p>If the adaptive parsimony strategy (<code>use_frequency</code> and <code>use_frequency_in_tournament</code>), this is how much to (exponentially) weight the contribution. If you find that the search is only optimizing the most complex expressions while the simpler expressions remain stagnant, you should increase this value.</p> <p>Default: <code>20.0</code></p> </li> <li> <p><code>should_simplify</code></p> <p>Whether to use algebraic simplification in the search. Note that only a few simple rules are implemented. </p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#mutations","title":"Mutations","text":"<ul> <li> <p><code>weight_add_node</code></p> <p>Relative likelihood for mutation to add a node.</p> <p>Default: <code>0.79</code></p> </li> <li> <p><code>weight_insert_node</code></p> <p>Relative likelihood for mutation to insert a node.</p> <p>Default: <code>5.1</code></p> </li> <li> <p><code>weight_delete_node</code></p> <p>Relative likelihood for mutation to delete a node.</p> <p>Default: <code>1.7</code></p> </li> <li> <p><code>weight_do_nothing</code></p> <p>Relative likelihood for mutation to leave the individual.</p> <p>Default: <code>0.21</code></p> </li> <li> <p><code>weight_mutate_constant</code></p> <p>Relative likelihood for mutation to change the constant slightly in a random direction.</p> <p>Default: <code>0.048</code></p> </li> <li> <p><code>weight_mutate_operator</code></p> <p>Relative likelihood for mutation to swap an operator.</p> <p>Default: <code>0.47</code></p> </li> <li> <p><code>weight_swap_operands</code></p> <p>Relative likehood for swapping operands in binary operators.</p> <p>Default: <code>0.1</code></p> </li> <li> <p><code>weight_randomize</code></p> <p>Relative likelihood for mutation to completely delete and then randomly generate the equation</p> <p>Default: <code>0.00023</code></p> </li> <li> <p><code>weight_simplify</code></p> <p>Relative likelihood for mutation to simplify constant parts by evaluation</p> <p>Default: <code>0.0020</code></p> </li> <li> <p><code>weight_optimize</code></p> <p>Constant optimization can also be performed as a mutation, in addition to the normal strategy controlled by <code>optimize_probability</code> which happens every iteration. Using it as a mutation is useful if you want to use a large <code>ncycles_periteration</code>, and may not optimize very often.</p> <p>Default: <code>0.0</code></p> </li> <li> <p><code>crossover_probability</code></p> <p>Absolute probability of crossover-type genetic operation, instead of a mutation.</p> <p>Default: <code>0.066</code></p> </li> <li> <p><code>annealing</code></p> <p>Whether to use annealing.  </p> <p>Default: <code>False</code></p> </li> <li> <p><code>alpha</code></p> <p>Initial temperature for simulated annealing (requires <code>annealing</code> to be <code>True</code>).</p> <p>Default: <code>0.1</code></p> </li> <li> <p><code>perturbation_factor</code></p> <p>Constants are perturbed by a max factor of (perturbation_factor*T + 1). Either multiplied by this or divided by this.</p> <p>Default: <code>0.076</code></p> </li> <li> <p><code>skip_mutation_failures</code></p> <p>Whether to skip mutation and crossover failures, rather than simply re-sampling the current member.</p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#tournament-selection","title":"Tournament Selection","text":"<ul> <li> <p><code>tournament_selection_n</code></p> <p>Number of expressions to consider in each tournament.</p> <p>Default: <code>10</code></p> </li> <li> <p><code>tournament_selection_p</code></p> <p>Probability of selecting the best expression in each tournament. The probability will decay as p*(1-p)^n for other expressions, sorted by loss.</p> <p>Default: <code>0.86</code></p> </li> </ul>"},{"location":"api/#constant-optimization","title":"Constant Optimization","text":"<ul> <li> <p><code>optimizer_algorithm</code></p> <p>Optimization scheme to use for optimizing constants. Can currently be <code>NelderMead</code> or <code>BFGS</code>.</p> <p>Default: <code>\"BFGS\"</code></p> </li> <li> <p><code>optimizer_nrestarts</code></p> <p>Number of time to restart the constants optimization process with different initial conditions.</p> <p>Default: <code>2</code></p> </li> <li> <p><code>optimize_probability</code></p> <p>Probability of optimizing the constants during a single iteration of the evolutionary algorithm.</p> <p>Default: <code>0.14</code></p> </li> <li> <p><code>optimizer_iterations</code></p> <p>Number of iterations that the constants optimizer can take.</p> <p>Default: <code>8</code></p> </li> <li> <p><code>should_optimize_constants</code></p> <p>Whether to numerically optimize constants (Nelder-Mead/Newton) at the end of each iteration. </p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#migration-between-populations","title":"Migration between Populations","text":"<ul> <li> <p><code>fraction_replaced</code></p> <p>How much of population to replace with migrating equations from other populations.</p> <p>Default: <code>0.000364</code></p> </li> <li> <p><code>fraction_replaced_hof</code></p> <p>How much of population to replace with migrating equations from hall of fame. </p> <p>Default: <code>0.035</code></p> </li> <li> <p><code>migration</code></p> <p>Whether to migrate.  </p> <p>Default: <code>True</code></p> </li> <li> <p><code>hof_migration</code></p> <p>Whether to have the hall of fame migrate.  </p> <p>Default: <code>True</code></p> </li> <li> <p><code>topn</code></p> <p>How many top individuals migrate from each population.</p> <p>Default: <code>12</code></p> </li> </ul>"},{"location":"api/#data-preprocessing","title":"Data Preprocessing","text":"<ul> <li> <p><code>denoise</code></p> <p>Whether to use a Gaussian Process to denoise the data before inputting to PySR. Can help PySR fit noisy data.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>select_k_features</code></p> <p>Whether to run feature selection in Python using random forests, before passing to the symbolic regression code. None means no feature selection; an int means select that many features.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#stopping-criteria","title":"Stopping Criteria","text":"<ul> <li> <p><code>max_evals</code></p> <p>Limits the total number of evaluations of expressions to this number.  </p> <p>Default: <code>None</code></p> </li> <li> <p><code>timeout_in_seconds</code></p> <p>Make the search return early once this many seconds have passed.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>early_stop_condition</code></p> <p>Stop the search early if this loss is reached. You may also pass a string containing a Julia function which takes a loss and complexity as input, for example: <code>\"f(loss, complexity) = (loss &lt; 0.1) &amp;&amp; (complexity &lt; 10)\"</code>.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#performance-and-parallelization","title":"Performance and Parallelization","text":"<ul> <li> <p><code>procs</code></p> <p>Number of processes (=number of populations running).</p> <p>Default: <code>cpu_count()</code></p> </li> <li> <p><code>multithreading</code></p> <p>Use multithreading instead of distributed backend. Using procs=0 will turn off both. </p> <p>Default: <code>True</code></p> </li> <li> <p><code>cluster_manager</code></p> <p>For distributed computing, this sets the job queue system. Set to one of \"slurm\", \"pbs\", \"lsf\", \"sge\", \"qrsh\", \"scyld\", or \"htc\". If set to one of these, PySR will run in distributed mode, and use <code>procs</code> to figure out how many processes to launch.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>heap_size_hint_in_bytes</code></p> <p>For multiprocessing, this sets the <code>--heap-size-hint</code> parameter for new Julia processes. This can be configured when using multi-node distributed compute, to give a hint to each process about how much memory they can use before aggressive garbage collection.</p> </li> <li> <p><code>batching</code></p> <p>Whether to compare population members on small batches during evolution. Still uses full dataset for comparing against hall of fame. </p> <p>Default: <code>False</code></p> </li> <li> <p><code>batch_size</code></p> <p>The amount of data to use if doing batching. </p> <p>Default: <code>50</code></p> </li> <li> <p><code>precision</code></p> <p>What precision to use for the data. By default this is <code>32</code> (float32), but you can select <code>64</code> or <code>16</code> as well, giving you 64 or 16 bits of floating point precision, respectively. If you pass complex data, the corresponding complex precision will be used (i.e., <code>64</code> for complex128, <code>32</code> for complex64).</p> <p>Default: <code>32</code></p> </li> <li> <p><code>fast_cycle</code></p> <p>Batch over population subsamples. This is a slightly different algorithm than regularized evolution, but does cycles 15% faster. May be algorithmically less efficient.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>turbo</code></p> <p>(Experimental) Whether to use LoopVectorization.jl to speed up the search evaluation. Certain operators may not be supported. Does not support 16-bit precision floats.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>bumper</code></p> <p>(Experimental) Whether to use Bumper.jl to speed up the search evaluation. Does not support 16-bit precision floats.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>enable_autodiff</code></p> <p>Whether to create derivative versions of operators for automatic differentiation. This is only necessary if you wish to compute the gradients of an expression within a custom loss function.</p> <p>Default: <code>False</code></p> </li> </ul>"},{"location":"api/#determinism","title":"Determinism","text":"<ul> <li> <p><code>random_state</code></p> <p>Pass an int for reproducible results across multiple function calls. See :term:<code>Glossary &lt;random_state&gt;</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>deterministic</code></p> <p>Make a PySR search give the same result every run. To use this, you must turn off parallelism (with <code>procs</code>=0, <code>multithreading</code>=False), and set <code>random_state</code> to a fixed seed.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>warm_start</code></p> <p>Tells fit to continue from where the last call to fit finished. If false, each call to fit will be fresh, overwriting previous results.</p> <p>Default: <code>False</code></p> </li> </ul>"},{"location":"api/#monitoring","title":"Monitoring","text":"<ul> <li> <p><code>verbosity</code></p> <p>What verbosity level to use. 0 means minimal print statements.</p> <p>Default: <code>1</code></p> </li> <li> <p><code>update_verbosity</code></p> <p>What verbosity level to use for package updates. Will take value of <code>verbosity</code> if not given.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>print_precision</code></p> <p>How many significant digits to print for floats. </p> <p>Default: <code>5</code></p> </li> <li> <p><code>progress</code></p> <p>Whether to use a progress bar instead of printing to stdout.</p> <p>Default: <code>True</code></p> </li> </ul>"},{"location":"api/#environment","title":"Environment","text":"<ul> <li> <p><code>temp_equation_file</code></p> <p>Whether to put the hall of fame file in the temp directory. Deletion is then controlled with the <code>delete_tempfiles</code> parameter.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>tempdir</code></p> <p>directory for the temporary files. </p> <p>Default: <code>None</code></p> </li> <li> <p><code>delete_tempfiles</code></p> <p>Whether to delete the temporary files after finishing.</p> <p>Default: <code>True</code></p> </li> <li> <p><code>update</code></p> <p>Whether to automatically update Julia packages when <code>fit</code> is called. You should make sure that PySR is up-to-date itself first, as the packaged Julia packages may not necessarily include all updated dependencies.</p> <p>Default: <code>False</code></p> </li> </ul>"},{"location":"api/#exporting-the-results","title":"Exporting the Results","text":"<ul> <li> <p><code>equation_file</code></p> <p>Where to save the files (.csv extension).</p> <p>Default: <code>None</code></p> </li> <li> <p><code>output_jax_format</code></p> <p>Whether to create a 'jax_format' column in the output, containing jax-callable functions and the default parameters in a jax array.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>output_torch_format</code></p> <p>Whether to create a 'torch_format' column in the output, containing a torch module with trainable parameters.</p> <p>Default: <code>False</code></p> </li> <li> <p><code>extra_sympy_mappings</code></p> <p>Provides mappings between custom <code>binary_operators</code> or <code>unary_operators</code> defined in julia strings, to those same operators defined in sympy. E.G if <code>unary_operators=[\"inv(x)=1/x\"]</code>, then for the fitted model to be export to sympy, <code>extra_sympy_mappings</code> would be <code>{\"inv\": lambda x: 1/x}</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>extra_torch_mappings</code></p> <p>The same as <code>extra_jax_mappings</code> but for model export to pytorch. Note that the dictionary keys should be callable pytorch expressions. For example: <code>extra_torch_mappings={sympy.sin: torch.sin}</code>.</p> <p>Default: <code>None</code></p> </li> <li> <p><code>extra_jax_mappings</code></p> <p>Similar to <code>extra_sympy_mappings</code> but for model export to jax. The dictionary maps sympy functions to jax functions. For example: <code>extra_jax_mappings={sympy.sin: \"jnp.sin\"}</code> maps the <code>sympy.sin</code> function to the equivalent jax expression <code>jnp.sin</code>.</p> <p>Default: <code>None</code></p> </li> </ul>"},{"location":"api/#pysrregressor-functions","title":"PySRRegressor Functions","text":""},{"location":"api/#pysr.PySRRegressor.fit","title":"<code>fit(X, y, Xresampled=None, weights=None, variable_names=None, complexity_of_variables=None, X_units=None, y_units=None)</code>","text":"<p>Search for equations to fit the dataset and store them in <code>self.equations_</code>.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape (n_samples, n_features).</p> required <code>y</code> <code>ndarray | DataFrame</code> <p>Target values of shape (n_samples,) or (n_samples, n_targets). Will be cast to X's dtype if necessary.</p> required <code>Xresampled</code> <code>ndarray | DataFrame</code> <p>Resampled training data, of shape (n_resampled, n_features), to generate a denoised data on. This will be used as the training data, rather than <code>X</code>.</p> <code>None</code> <code>weights</code> <code>ndarray | DataFrame</code> <p>Weight array of the same shape as <code>y</code>. Each element is how to weight the mean-square-error loss for that particular element of <code>y</code>. Alternatively, if a custom <code>loss</code> was set, it will can be used in arbitrary ways.</p> <code>None</code> <code>variable_names</code> <code>list[str]</code> <p>A list of names for the variables, rather than \"x0\", \"x1\", etc. If <code>X</code> is a pandas dataframe, the column names will be used instead of <code>variable_names</code>. Cannot contain spaces or special characters. Avoid variable names which are also function names in <code>sympy</code>, such as \"N\".</p> <code>None</code> <code>X_units</code> <code>list[str]</code> <p>A list of units for each variable in <code>X</code>. Each unit should be a string representing a Julia expression. See DynamicQuantities.jl https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more information.</p> <code>None</code> <code>y_units</code> <code>str | list[str]</code> <p>Similar to <code>X_units</code>, but as a unit for the target variable, <code>y</code>. If <code>y</code> is a matrix, a list of units should be passed. If <code>X_units</code> is given but <code>y_units</code> is not, then <code>y_units</code> will be arbitrary.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>self</code> <code>object</code> <p>Fitted estimator.</p> Source code in <code>pysr/sr.py</code> <pre><code>def fit(\n    self,\n    X,\n    y,\n    Xresampled=None,\n    weights=None,\n    variable_names: Optional[ArrayLike[str]] = None,\n    complexity_of_variables: Optional[\n        Union[int, float, List[Union[int, float]]]\n    ] = None,\n    X_units: Optional[ArrayLike[str]] = None,\n    y_units: Optional[Union[str, ArrayLike[str]]] = None,\n) -&gt; \"PySRRegressor\":\n    \"\"\"\n    Search for equations to fit the dataset and store them in `self.equations_`.\n\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape (n_samples, n_features).\n    y : ndarray | pandas.DataFrame\n        Target values of shape (n_samples,) or (n_samples, n_targets).\n        Will be cast to X's dtype if necessary.\n    Xresampled : ndarray | pandas.DataFrame\n        Resampled training data, of shape (n_resampled, n_features),\n        to generate a denoised data on. This\n        will be used as the training data, rather than `X`.\n    weights : ndarray | pandas.DataFrame\n        Weight array of the same shape as `y`.\n        Each element is how to weight the mean-square-error loss\n        for that particular element of `y`. Alternatively,\n        if a custom `loss` was set, it will can be used\n        in arbitrary ways.\n    variable_names : list[str]\n        A list of names for the variables, rather than \"x0\", \"x1\", etc.\n        If `X` is a pandas dataframe, the column names will be used\n        instead of `variable_names`. Cannot contain spaces or special\n        characters. Avoid variable names which are also\n        function names in `sympy`, such as \"N\".\n    X_units : list[str]\n        A list of units for each variable in `X`. Each unit should be\n        a string representing a Julia expression. See DynamicQuantities.jl\n        https://symbolicml.org/DynamicQuantities.jl/dev/units/ for more\n        information.\n    y_units : str | list[str]\n        Similar to `X_units`, but as a unit for the target variable, `y`.\n        If `y` is a matrix, a list of units should be passed. If `X_units`\n        is given but `y_units` is not, then `y_units` will be arbitrary.\n\n    Returns\n    -------\n    self : object\n        Fitted estimator.\n    \"\"\"\n    # Init attributes that are not specified in BaseEstimator\n    if self.warm_start and hasattr(self, \"julia_state_stream_\"):\n        pass\n    else:\n        if hasattr(self, \"julia_state_stream_\"):\n            warnings.warn(\n                \"The discovered expressions are being reset. \"\n                \"Please set `warm_start=True` if you wish to continue \"\n                \"to start a search where you left off.\",\n            )\n\n        self.equations_ = None\n        self.nout_ = 1\n        self.selection_mask_ = None\n        self.julia_state_stream_ = None\n        self.julia_options_stream_ = None\n        self.complexity_of_variables_ = None\n        self.X_units_ = None\n        self.y_units_ = None\n\n    self._setup_equation_file()\n\n    runtime_params = self._validate_and_modify_params()\n\n    (\n        X,\n        y,\n        Xresampled,\n        weights,\n        variable_names,\n        complexity_of_variables,\n        X_units,\n        y_units,\n    ) = self._validate_and_set_fit_params(\n        X,\n        y,\n        Xresampled,\n        weights,\n        variable_names,\n        complexity_of_variables,\n        X_units,\n        y_units,\n    )\n\n    if X.shape[0] &gt; 10000 and not self.batching:\n        warnings.warn(\n            \"Note: you are running with more than 10,000 datapoints. \"\n            \"You should consider turning on batching (https://astroautomata.com/PySR/options/#batching). \"\n            \"You should also reconsider if you need that many datapoints. \"\n            \"Unless you have a large amount of noise (in which case you \"\n            \"should smooth your dataset first), generally &lt; 10,000 datapoints \"\n            \"is enough to find a functional form with symbolic regression. \"\n            \"More datapoints will lower the search speed.\"\n        )\n\n    random_state = check_random_state(self.random_state)  # For np random\n    seed = cast(int, random_state.randint(0, 2**31 - 1))  # For julia random\n\n    # Pre transformations (feature selection and denoising)\n    X, y, variable_names, complexity_of_variables, X_units, y_units = (\n        self._pre_transform_training_data(\n            X,\n            y,\n            Xresampled,\n            variable_names,\n            complexity_of_variables,\n            X_units,\n            y_units,\n            random_state,\n        )\n    )\n\n    # Warn about large feature counts (still warn if feature count is large\n    # after running feature selection)\n    if self.n_features_in_ &gt;= 10:\n        warnings.warn(\n            \"Note: you are running with 10 features or more. \"\n            \"Genetic algorithms like used in PySR scale poorly with large numbers of features. \"\n            \"You should run PySR for more `niterations` to ensure it can find \"\n            \"the correct variables, and consider using a larger `maxsize`.\"\n        )\n\n    # Assertion checks\n    use_custom_variable_names = variable_names is not None\n    # TODO: this is always true.\n\n    _check_assertions(\n        X,\n        use_custom_variable_names,\n        variable_names,\n        complexity_of_variables,\n        weights,\n        y,\n        X_units,\n        y_units,\n    )\n\n    # Initially, just save model parameters, so that\n    # it can be loaded from an early exit:\n    if not self.temp_equation_file:\n        self._checkpoint()\n\n    # Perform the search:\n    self._run(X, y, runtime_params, weights=weights, seed=seed)\n\n    # Then, after fit, we save again, so the pickle file contains\n    # the equations:\n    if not self.temp_equation_file:\n        self._checkpoint()\n\n    return self\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.predict","title":"<code>predict(X, index=None)</code>","text":"<p>Predict y from input X using the equation chosen by <code>model_selection</code>.</p> <p>You may see what equation is used by printing this object. X should have the same columns as the training data.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray | DataFrame</code> <p>Training data of shape <code>(n_samples, n_features)</code>.</p> required <code>index</code> <code>int | list[int]</code> <p>If you want to compute the output of an expression using a particular row of <code>self.equations_</code>, you may specify the index here. For multiple output equations, you must pass a list of indices in the same order.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>y_predicted</code> <code>ndarray of shape (n_samples, nout_)</code> <p>Values predicted by substituting <code>X</code> into the fitted symbolic regression model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>Raises if the <code>best_equation</code> cannot be evaluated.</p> Source code in <code>pysr/sr.py</code> <pre><code>def predict(self, X, index=None):\n    \"\"\"\n    Predict y from input X using the equation chosen by `model_selection`.\n\n    You may see what equation is used by printing this object. X should\n    have the same columns as the training data.\n\n    Parameters\n    ----------\n    X : ndarray | pandas.DataFrame\n        Training data of shape `(n_samples, n_features)`.\n    index : int | list[int]\n        If you want to compute the output of an expression using a\n        particular row of `self.equations_`, you may specify the index here.\n        For multiple output equations, you must pass a list of indices\n        in the same order.\n\n    Returns\n    -------\n    y_predicted : ndarray of shape (n_samples, nout_)\n        Values predicted by substituting `X` into the fitted symbolic\n        regression model.\n\n    Raises\n    ------\n    ValueError\n        Raises if the `best_equation` cannot be evaluated.\n    \"\"\"\n    check_is_fitted(\n        self, attributes=[\"selection_mask_\", \"feature_names_in_\", \"nout_\"]\n    )\n    best_equation = self.get_best(index=index)\n\n    # When X is an numpy array or a pandas dataframe with a RangeIndex,\n    # the self.feature_names_in_ generated during fit, for the same X,\n    # will cause a warning to be thrown during _validate_data.\n    # To avoid this, convert X to a dataframe, apply the selection mask,\n    # and then set the column/feature_names of X to be equal to those\n    # generated during fit.\n    if not isinstance(X, pd.DataFrame):\n        X = check_array(X)\n        X = pd.DataFrame(X)\n    if isinstance(X.columns, pd.RangeIndex):\n        if self.selection_mask_ is not None:\n            # RangeIndex enforces column order allowing columns to\n            # be correctly filtered with self.selection_mask_\n            X = X[X.columns[self.selection_mask_]]\n        X.columns = self.feature_names_in_\n    # Without feature information, CallableEquation/lambda_format equations\n    # require that the column order of X matches that of the X used during\n    # the fitting process. _validate_data removes this feature information\n    # when it converts the dataframe to an np array. Thus, to ensure feature\n    # order is preserved after conversion, the dataframe columns must be\n    # reordered/reindexed to match those of the transformed (denoised and\n    # feature selected) X in fit.\n    X = X.reindex(columns=self.feature_names_in_)\n    X = self._validate_data_X(X)\n\n    try:\n        if isinstance(best_equation, list):\n            assert self.nout_ &gt; 1\n            return np.stack(\n                [eq[\"lambda_format\"](X) for eq in best_equation], axis=1\n            )\n        else:\n            return best_equation[\"lambda_format\"](X)\n    except Exception as error:\n        raise ValueError(\n            \"Failed to evaluate the expression. \"\n            \"If you are using a custom operator, make sure to define it in `extra_sympy_mappings`, \"\n            \"e.g., `model.set_params(extra_sympy_mappings={'inv': lambda x: 1/x})`, where \"\n            \"`lambda x: 1/x` is a valid SymPy function defining the operator. \"\n            \"You can then run `model.refresh()` to re-load the expressions.\"\n        ) from error\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.from_file","title":"<code>from_file(equation_file, *, binary_operators=None, unary_operators=None, n_features_in=None, feature_names_in=None, selection_mask=None, nout=1, **pysr_kwargs)</code>  <code>classmethod</code>","text":"<p>Create a model from a saved model checkpoint or equation file.</p> <p>Parameters:</p> Name Type Description Default <code>equation_file</code> <code>str or Path</code> <p>Path to a pickle file containing a saved model, or a csv file containing equations.</p> required <code>binary_operators</code> <code>list[str]</code> <p>The same binary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>unary_operators</code> <code>list[str]</code> <p>The same unary operators used when creating the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>n_features_in</code> <code>int</code> <p>Number of features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>feature_names_in</code> <code>list[str]</code> <p>Names of the features passed to the model. Not needed if loading from a pickle file.</p> <code>None</code> <code>selection_mask</code> <code>NDArray[bool_]</code> <p>If using <code>select_k_features</code>, you must pass <code>model.selection_mask_</code> here. Not needed if loading from a pickle file.</p> <code>None</code> <code>nout</code> <code>int</code> <p>Number of outputs of the model. Not needed if loading from a pickle file. Default is <code>1</code>.</p> <code>1</code> <code>**pysr_kwargs</code> <code>dict</code> <p>Any other keyword arguments to initialize the PySRRegressor object. These will overwrite those stored in the pickle file. Not needed if loading from a pickle file.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>model</code> <code>PySRRegressor</code> <p>The model with fitted equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>@classmethod\ndef from_file(\n    cls,\n    equation_file: PathLike,\n    *,\n    binary_operators: Optional[List[str]] = None,\n    unary_operators: Optional[List[str]] = None,\n    n_features_in: Optional[int] = None,\n    feature_names_in: Optional[ArrayLike[str]] = None,\n    selection_mask: Optional[NDArray[np.bool_]] = None,\n    nout: int = 1,\n    **pysr_kwargs,\n):\n    \"\"\"\n    Create a model from a saved model checkpoint or equation file.\n\n    Parameters\n    ----------\n    equation_file : str or Path\n        Path to a pickle file containing a saved model, or a csv file\n        containing equations.\n    binary_operators : list[str]\n        The same binary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    unary_operators : list[str]\n        The same unary operators used when creating the model.\n        Not needed if loading from a pickle file.\n    n_features_in : int\n        Number of features passed to the model.\n        Not needed if loading from a pickle file.\n    feature_names_in : list[str]\n        Names of the features passed to the model.\n        Not needed if loading from a pickle file.\n    selection_mask : NDArray[np.bool_]\n        If using `select_k_features`, you must pass `model.selection_mask_` here.\n        Not needed if loading from a pickle file.\n    nout : int\n        Number of outputs of the model.\n        Not needed if loading from a pickle file.\n        Default is `1`.\n    **pysr_kwargs : dict\n        Any other keyword arguments to initialize the PySRRegressor object.\n        These will overwrite those stored in the pickle file.\n        Not needed if loading from a pickle file.\n\n    Returns\n    -------\n    model : PySRRegressor\n        The model with fitted equations.\n    \"\"\"\n\n    pkl_filename = _csv_filename_to_pkl_filename(equation_file)\n\n    # Try to load model from &lt;equation_file&gt;.pkl\n    print(f\"Checking if {pkl_filename} exists...\")\n    if os.path.exists(pkl_filename):\n        print(f\"Loading model from {pkl_filename}\")\n        assert binary_operators is None\n        assert unary_operators is None\n        assert n_features_in is None\n        with open(pkl_filename, \"rb\") as f:\n            model = pkl.load(f)\n        # Change equation_file_ to be in the same dir as the pickle file\n        base_dir = os.path.dirname(pkl_filename)\n        base_equation_file = os.path.basename(model.equation_file_)\n        model.equation_file_ = os.path.join(base_dir, base_equation_file)\n\n        # Update any parameters if necessary, such as\n        # extra_sympy_mappings:\n        model.set_params(**pysr_kwargs)\n        if \"equations_\" not in model.__dict__ or model.equations_ is None:\n            model.refresh()\n\n        return model\n\n    # Else, we re-create it.\n    print(\n        f\"{pkl_filename} does not exist, \"\n        \"so we must create the model from scratch.\"\n    )\n    assert binary_operators is not None or unary_operators is not None\n    assert n_features_in is not None\n\n    # TODO: copy .bkup file if exists.\n    model = cls(\n        equation_file=str(equation_file),\n        binary_operators=binary_operators,\n        unary_operators=unary_operators,\n        **pysr_kwargs,\n    )\n\n    model.nout_ = nout\n    model.n_features_in_ = n_features_in\n\n    if feature_names_in is None:\n        model.feature_names_in_ = np.array([f\"x{i}\" for i in range(n_features_in)])\n        model.display_feature_names_in_ = np.array(\n            [f\"x{_subscriptify(i)}\" for i in range(n_features_in)]\n        )\n    else:\n        assert len(feature_names_in) == n_features_in\n        model.feature_names_in_ = feature_names_in\n        model.display_feature_names_in_ = feature_names_in\n\n    if selection_mask is None:\n        model.selection_mask_ = np.ones(n_features_in, dtype=np.bool_)\n    else:\n        model.selection_mask_ = selection_mask\n\n    model.refresh(checkpoint_file=equation_file)\n\n    return model\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.sympy","title":"<code>sympy(index=None)</code>","text":"<p>Return sympy representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str, list[str] of length nout_</code> <p>SymPy representation of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def sympy(self, index=None):\n    \"\"\"\n    Return sympy representation of the equation(s) chosen by `model_selection`.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n\n    Returns\n    -------\n    best_equation : str, list[str] of length nout_\n        SymPy representation of the best equation.\n    \"\"\"\n    self.refresh()\n    best_equation = self.get_best(index=index)\n    if isinstance(best_equation, list):\n        assert self.nout_ &gt; 1\n        return [eq[\"sympy_format\"] for eq in best_equation]\n    else:\n        return best_equation[\"sympy_format\"]\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.latex","title":"<code>latex(index=None, precision=3)</code>","text":"<p>Return latex representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representation. Default is <code>3</code>.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>str or list[str] of length nout_</code> <p>LaTeX expression of the best equation.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex(self, index=None, precision=3):\n    \"\"\"\n    Return latex representation of the equation(s) chosen by `model_selection`.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representation.\n        Default is `3`.\n\n    Returns\n    -------\n    best_equation : str or list[str] of length nout_\n        LaTeX expression of the best equation.\n    \"\"\"\n    self.refresh()\n    sympy_representation = self.sympy(index=index)\n    if self.nout_ &gt; 1:\n        output = []\n        for s in sympy_representation:\n            latex = sympy2latex(s, prec=precision)\n            output.append(latex)\n        return output\n    return sympy2latex(sympy_representation, prec=precision)\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.pytorch","title":"<code>pytorch(index=None)</code>","text":"<p>Return pytorch representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a PyTorch module containing the parameters as trainable attributes. You can use the module like any other PyTorch module: <code>module(X)</code>, where <code>X</code> is a tensor with the same column ordering as trained with.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>Module</code> <p>PyTorch module representing the expression.</p> Source code in <code>pysr/sr.py</code> <pre><code>def pytorch(self, index=None):\n    \"\"\"\n    Return pytorch representation of the equation(s) chosen by `model_selection`.\n\n    Each equation (multiple given if there are multiple outputs) is a PyTorch module\n    containing the parameters as trainable attributes. You can use the module like\n    any other PyTorch module: `module(X)`, where `X` is a tensor with the same\n    column ordering as trained with.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n\n    Returns\n    -------\n    best_equation : torch.nn.Module\n        PyTorch module representing the expression.\n    \"\"\"\n    self.set_params(output_torch_format=True)\n    self.refresh()\n    best_equation = self.get_best(index=index)\n    if isinstance(best_equation, list):\n        return [eq[\"torch_format\"] for eq in best_equation]\n    else:\n        return best_equation[\"torch_format\"]\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.jax","title":"<code>jax(index=None)</code>","text":"<p>Return jax representation of the equation(s) chosen by <code>model_selection</code>.</p> <p>Each equation (multiple given if there are multiple outputs) is a dictionary containing {\"callable\": func, \"parameters\": params}. To call <code>func</code>, pass func(X, params). This function is differentiable using <code>jax.grad</code>.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>int | list[int]</code> <p>If you wish to select a particular equation from <code>self.equations_</code>, give the index number here. This overrides the <code>model_selection</code> parameter. If there are multiple output features, then pass a list of indices with the order the same as the output feature.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>best_equation</code> <code>dict[str, Any]</code> <p>Dictionary of callable jax function in \"callable\" key, and jax array of parameters as \"parameters\" key.</p> Source code in <code>pysr/sr.py</code> <pre><code>def jax(self, index=None):\n    \"\"\"\n    Return jax representation of the equation(s) chosen by `model_selection`.\n\n    Each equation (multiple given if there are multiple outputs) is a dictionary\n    containing {\"callable\": func, \"parameters\": params}. To call `func`, pass\n    func(X, params). This function is differentiable using `jax.grad`.\n\n    Parameters\n    ----------\n    index : int | list[int]\n        If you wish to select a particular equation from\n        `self.equations_`, give the index number here. This overrides\n        the `model_selection` parameter. If there are multiple output\n        features, then pass a list of indices with the order the same\n        as the output feature.\n\n    Returns\n    -------\n    best_equation : dict[str, Any]\n        Dictionary of callable jax function in \"callable\" key,\n        and jax array of parameters as \"parameters\" key.\n    \"\"\"\n    self.set_params(output_jax_format=True)\n    self.refresh()\n    best_equation = self.get_best(index=index)\n    if isinstance(best_equation, list):\n        assert self.nout_ &gt; 1\n        return [eq[\"jax_format\"] for eq in best_equation]\n    else:\n        return best_equation[\"jax_format\"]\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.latex_table","title":"<code>latex_table(indices=None, precision=3, columns=['equation', 'complexity', 'loss', 'score'])</code>","text":"<p>Create a LaTeX/booktabs table for all, or some, of the equations.</p> <p>Parameters:</p> Name Type Description Default <code>indices</code> <code>list[int] | list[list[int]]</code> <p>If you wish to select a particular subset of equations from <code>self.equations_</code>, give the row numbers here. By default, all equations will be used. If there are multiple output features, then pass a list of lists.</p> <code>None</code> <code>precision</code> <code>int</code> <p>The number of significant figures shown in the LaTeX representations. Default is <code>3</code>.</p> <code>3</code> <code>columns</code> <code>list[str]</code> <p>Which columns to include in the table. Default is <code>[\"equation\", \"complexity\", \"loss\", \"score\"]</code>.</p> <code>['equation', 'complexity', 'loss', 'score']</code> <p>Returns:</p> Name Type Description <code>latex_table_str</code> <code>str</code> <p>A string that will render a table in LaTeX of the equations.</p> Source code in <code>pysr/sr.py</code> <pre><code>def latex_table(\n    self,\n    indices=None,\n    precision=3,\n    columns=[\"equation\", \"complexity\", \"loss\", \"score\"],\n):\n    \"\"\"Create a LaTeX/booktabs table for all, or some, of the equations.\n\n    Parameters\n    ----------\n    indices : list[int] | list[list[int]]\n        If you wish to select a particular subset of equations from\n        `self.equations_`, give the row numbers here. By default,\n        all equations will be used. If there are multiple output\n        features, then pass a list of lists.\n    precision : int\n        The number of significant figures shown in the LaTeX\n        representations.\n        Default is `3`.\n    columns : list[str]\n        Which columns to include in the table.\n        Default is `[\"equation\", \"complexity\", \"loss\", \"score\"]`.\n\n    Returns\n    -------\n    latex_table_str : str\n        A string that will render a table in LaTeX of the equations.\n    \"\"\"\n    self.refresh()\n\n    if isinstance(self.equations_, list):\n        if indices is not None:\n            assert isinstance(indices, list)\n            assert isinstance(indices[0], list)\n            assert len(indices) == self.nout_\n\n        table_string = sympy2multilatextable(\n            self.equations_, indices=indices, precision=precision, columns=columns\n        )\n    elif isinstance(self.equations_, pd.DataFrame):\n        if indices is not None:\n            assert isinstance(indices, list)\n            assert isinstance(indices[0], int)\n\n        table_string = sympy2latextable(\n            self.equations_, indices=indices, precision=precision, columns=columns\n        )\n    else:\n        raise ValueError(\n            \"Invalid type for equations_ to pass to `latex_table`. \"\n            \"Expected a DataFrame or a list of DataFrames.\"\n        )\n\n    return with_preamble(table_string)\n</code></pre>"},{"location":"api/#pysr.PySRRegressor.refresh","title":"<code>refresh(checkpoint_file=None)</code>","text":"<p>Update self.equations_ with any new options passed.</p> <p>For example, updating <code>extra_sympy_mappings</code> will require a <code>.refresh()</code> to update the equations.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint_file</code> <code>str or Path</code> <p>Path to checkpoint hall of fame file to be loaded. The default will use the set <code>equation_file_</code>.</p> <code>None</code> Source code in <code>pysr/sr.py</code> <pre><code>def refresh(self, checkpoint_file: Optional[PathLike] = None) -&gt; None:\n    \"\"\"\n    Update self.equations_ with any new options passed.\n\n    For example, updating `extra_sympy_mappings`\n    will require a `.refresh()` to update the equations.\n\n    Parameters\n    ----------\n    checkpoint_file : str or Path\n        Path to checkpoint hall of fame file to be loaded.\n        The default will use the set `equation_file_`.\n    \"\"\"\n    if checkpoint_file is not None:\n        self.equation_file_ = checkpoint_file\n        self.equation_file_contents_ = None\n    check_is_fitted(self, attributes=[\"equation_file_\"])\n    self.equations_ = self.get_hof()\n</code></pre>"},{"location":"backend/","title":"Customization","text":"<p>If you have explored the options and PySRRegressor reference, and still haven't figured out how to specify a constraint or objective required for your problem, you might consider editing the backend. The backend of PySR is written as a pure Julia package under the name SymbolicRegression.jl. This package is accessed with <code>juliacall</code>, which allows us to transfer objects back and forth between the Python and Julia runtimes.</p> <p>PySR gives you access to everything in SymbolicRegression.jl, but there are some specific use-cases which require modifications to the backend itself. Generally you can do this as follows:</p>"},{"location":"backend/#1-check-out-the-source-code","title":"1. Check out the source code","text":"<p>Clone a copy of the backend as well as PySR:</p> <pre><code>git clone https://github.com/MilesCranmer/SymbolicRegression.jl\ngit clone https://github.com/MilesCranmer/PySR\n</code></pre> <p>You may wish to check out the specific versions, which you can do with:</p> <pre><code>cd PySR\ngit checkout &lt;version&gt;\n\n# You can see the current backend version in `pysr/juliapkg.json`\ncd ../SymbolicRegression.jl\ngit checkout &lt;backend_version&gt;\n</code></pre>"},{"location":"backend/#2-edit-the-source-to-your-requirements","title":"2. Edit the source to your requirements","text":"<p>The main search code can be found in <code>src/SymbolicRegression.jl</code>.</p> <p>Here are some tips:</p> <ul> <li>The documentation for the backend is given here.</li> <li>Throughout the package, you will often see template functions which typically use a symbol <code>T</code> (such as in the string <code>where {T&lt;:Real}</code>). Here, <code>T</code> is simply the datatype of the input data and stored constants, such as <code>Float32</code> or <code>Float64</code>. Writing functions in this way lets us write functions generic to types, while still having access to the specific type specified at compilation time.</li> <li>Expressions are stored as binary trees, using the <code>Node{T}</code> type, described here.</li> <li>For reference, the main loop itself is found in the <code>equation_search</code> function inside <code>src/SymbolicRegression.jl</code>.</li> <li>Parts of the code which are typically edited by users include:<ul> <li><code>src/CheckConstraints.jl</code>, particularly the function <code>check_constraints</code>. This function checks whether a given expression satisfies constraints, such as having a complexity lower than <code>maxsize</code>, and whether it contains any forbidden nestings of functions.<ul> <li>Note that all expressions, even intermediate expressions, must comply with constraints. Therefore, make sure that evolution can still reach your desired expression (with one mutation at a time), before setting a hard constraint. In other cases you might want to instead put in the loss function.</li> </ul> </li> <li><code>src/Options.jl</code>, as well as the struct definition in <code>src/OptionsStruct.jl</code>. This file specifies all the options used in the search: an instance of <code>Options</code> is typically available throughout every function in <code>SymbolicRegression.jl</code>. If you add new functionality to the backend, and wish to make it parameterizable (including from PySR), you should specify it in the options.</li> </ul> </li> </ul>"},{"location":"backend/#3-let-pysr-use-the-modified-backend","title":"3. Let PySR use the modified backend","text":"<p>Once you have made your changes, you should edit the <code>pysr/juliapkg.json</code> file in the PySR repository to point to this local copy. Do this by removing the <code>\"version\"</code> key and adding a <code>\"dev\"</code> and <code>\"path\"</code> key:</p> <pre><code>    ...\n    \"packages\": {\n        \"SymbolicRegression\": {\n            \"uuid\": \"8254be44-1295-4e6a-a16d-46603ac705cb\",\n            \"dev\": true,\n            \"path\": \"/path/to/SymbolicRegression.jl\"\n        },\n    ...\n</code></pre> <p>You can then install PySR with this modified backend by running:</p> <pre><code>cd PySR\npip install .\n</code></pre> <p>For more information on <code>juliapkg.json</code>, see <code>pyjuliapkg</code>.</p>"},{"location":"backend/#additional-notes","title":"Additional notes","text":"<p>If you get comfortable enough with the backend, you might consider using the Julia package directly: the API is given on the SymbolicRegression.jl documentation.</p> <p>If you make a change that you think could be useful to other users, don't hesitate to open a pull request on either the PySR or SymbolicRegression.jl repositories! Contributions are very appreciated.</p>"},{"location":"examples/","title":"Toy Examples with Code","text":""},{"location":"examples/#preamble","title":"Preamble","text":"<pre><code>import numpy as np\nfrom pysr import *\n</code></pre>"},{"location":"examples/#1-simple-search","title":"1. Simple search","text":"<p>Here's a simple example where we find the expression <code>2 cos(x3) + x0^2 - 2</code>.</p> <pre><code>X = 2 * np.random.randn(100, 5)\ny = 2 * np.cos(X[:, 3]) + X[:, 0] ** 2 - 2\nmodel = PySRRegressor(binary_operators=[\"+\", \"-\", \"*\", \"/\"])\nmodel.fit(X, y)\nprint(model)\n</code></pre>"},{"location":"examples/#2-custom-operator","title":"2. Custom operator","text":"<p>Here, we define a custom operator and use it to find an expression:</p> <pre><code>X = 2 * np.random.randn(100, 5)\ny = 1 / X[:, 0]\nmodel = PySRRegressor(\n    binary_operators=[\"+\", \"*\"],\n    unary_operators=[\"inv(x) = 1/x\"],\n    extra_sympy_mappings={\"inv\": lambda x: 1/x},\n)\nmodel.fit(X, y)\nprint(model)\n</code></pre>"},{"location":"examples/#3-multiple-outputs","title":"3. Multiple outputs","text":"<p>Here, we do the same thing, but with multiple expressions at once, each requiring a different feature.</p> <pre><code>X = 2 * np.random.randn(100, 5)\ny = 1 / X[:, [0, 1, 2]]\nmodel = PySRRegressor(\n    binary_operators=[\"+\", \"*\"],\n    unary_operators=[\"inv(x) = 1/x\"],\n    extra_sympy_mappings={\"inv\": lambda x: 1/x},\n)\nmodel.fit(X, y)\n</code></pre>"},{"location":"examples/#4-plotting-an-expression","title":"4. Plotting an expression","text":"<p>For now, let's consider the expressions for output 0. We can see the LaTeX version of this with:</p> <pre><code>model.latex()[0]\n</code></pre> <p>or output 1 with <code>model.latex()[1]</code>.</p> <p>Let's plot the prediction against the truth:</p> <pre><code>from matplotlib import pyplot as plt\nplt.scatter(y[:, 0], model.predict(X)[:, 0])\nplt.xlabel('Truth')\nplt.ylabel('Prediction')\nplt.show()\n</code></pre> <p>Which gives us:</p> <p></p> <p>We may also plot the output of a particular expression by passing the index of the expression to <code>predict</code> (or <code>sympy</code> or <code>latex</code> as well)</p>"},{"location":"examples/#5-feature-selection","title":"5. Feature selection","text":"<p>PySR and evolution-based symbolic regression in general performs very poorly when the number of features is large. Even, say, 10 features might be too much for a typical equation search.</p> <p>If you are dealing with high-dimensional data with a particular type of structure, you might consider using deep learning to break the problem into smaller \"chunks\" which can then be solved by PySR, as explained in the paper 2006.11287.</p> <p>For tabular datasets, this is a bit trickier. Luckily, PySR has a built-in feature selection mechanism. Simply declare the parameter <code>select_k_features=5</code>, for selecting the most important 5 features.</p> <p>Here is an example. Let's say we have 30 input features and 300 data points, but only 2 of those features are actually used:</p> <pre><code>X = np.random.randn(300, 30)\ny = X[:, 3]**2 - X[:, 19]**2 + 1.5\n</code></pre> <p>Let's create a model with the feature selection argument set up:</p> <pre><code>model = PySRRegressor(\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"exp\"],\n    select_k_features=5,\n)\n</code></pre> <p>Now let's fit this:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>Before the Julia backend is launched, you can see the string:</p> <pre><code>Using features ['x3', 'x5', 'x7', 'x19', 'x21']\n</code></pre> <p>which indicates that the feature selection (powered by a gradient-boosting tree) has successfully selected the relevant two features.</p> <p>This fit should find the solution quickly, whereas with the huge number of features, it would have struggled.</p> <p>This simple preprocessing step is enough to simplify our tabular dataset, but again, for more structured datasets, you should try the deep learning approach mentioned above.</p>"},{"location":"examples/#6-denoising","title":"6. Denoising","text":"<p>Many datasets, especially in the observational sciences, contain intrinsic noise. PySR is noise robust itself, as it is simply optimizing a loss function, but there are still some additional steps you can take to reduce the effect of noise.</p> <p>One thing you could do, which we won't detail here, is to create a custom log-likelihood given some assumed noise model. By passing weights to the fit function, and defining a custom loss function such as <code>elementwise_loss=\"myloss(x, y, w) = w * (x - y)^2\"</code>, you can define any sort of log-likelihood you wish. (However, note that it must be bounded at zero)</p> <p>However, the simplest thing to do is preprocessing, just like for feature selection. To do this, set the parameter <code>denoise=True</code>. This will fit a Gaussian process (containing a white noise kernel) to the input dataset, and predict new targets (which are assumed to be denoised) from that Gaussian process.</p> <p>For example:</p> <pre><code>X = np.random.randn(100, 5)\nnoise = np.random.randn(100) * 0.1\ny = np.exp(X[:, 0]) + X[:, 1] + X[:, 2] + noise\n</code></pre> <p>Let's create and fit a model with the denoising argument set up:</p> <pre><code>model = PySRRegressor(\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"exp\"],\n    denoise=True,\n)\nmodel.fit(X, y)\nprint(model)\n</code></pre> <p>If all goes well, you should find that it predicts the correct input equation, without the noise term!</p>"},{"location":"examples/#7-julia-packages-and-types","title":"7. Julia packages and types","text":"<p>PySR uses SymbolicRegression.jl as its search backend. This is a pure Julia package, and so can interface easily with any other Julia package. For some tasks, it may be necessary to load such a package.</p> <p>For example, let's say we wish to discovery the following relationship:</p> \\[ y = p_{3x + 1} - 5, \\] <p>where \\(p_i\\) is the \\(i\\)th prime number, and \\(x\\) is the input feature.</p> <p>Let's see if we can discover this using the Primes.jl package.</p> <p>First, let's get the Julia backend:</p> <pre><code>from pysr import jl\n</code></pre> <p><code>jl</code> stores the Julia runtime.</p> <p>Now, let's run some Julia code to add the Primes.jl package to the PySR environment:</p> <pre><code>jl.seval(\"\"\"\nimport Pkg\nPkg.add(\"Primes\")\n\"\"\")\n</code></pre> <p>This imports the Julia package manager, and uses it to install <code>Primes.jl</code>. Now let's import <code>Primes.jl</code>:</p> <pre><code>jl.seval(\"import Primes\")\n</code></pre> <p>Now, we define a custom operator:</p> <pre><code>jl.seval(\"\"\"\nfunction p(i::T) where T\n    if (0.5 &lt; i &lt; 1000)\n        return T(Primes.prime(round(Int, i)))\n    else\n        return T(NaN)\n    end\nend\n\"\"\")\n</code></pre> <p>We have created a a function <code>p</code>, which takes an arbitrary number as input. <code>p</code> first checks whether the input is between 0.5 and 1000. If out-of-bounds, it returns <code>NaN</code>. If in-bounds, it rounds it to the nearest integer, compures the corresponding prime number, and then converts it to the same type as input.</p> <p>Next, let's generate a list of primes for our test dataset. Since we are using juliacall, we can just call <code>p</code> directly to do this:</p> <pre><code>primes = {i: jl.p(i*1.0) for i in range(1, 999)}\n</code></pre> <p>Next, let's use this list of primes to create a dataset of \\(x, y\\) pairs:</p> <pre><code>import numpy as np\n\nX = np.random.randint(0, 100, 100)[:, None]\ny = [primes[3*X[i, 0] + 1] - 5 + np.random.randn()*0.001 for i in range(100)]\n</code></pre> <p>Note that we have also added a tiny bit of noise to the dataset.</p> <p>Finally, let's create a PySR model, and pass the custom operator. We also need to define the sympy equivalent, which we can leave as a placeholder for now:</p> <pre><code>from pysr import PySRRegressor\nimport sympy\n\nclass sympy_p(sympy.Function):\n    pass\n\nmodel = PySRRegressor(\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"p\"],\n    niterations=100,\n    extra_sympy_mappings={\"p\": sympy_p}\n)\n</code></pre> <p>We are all set to go! Let's see if we can find the true relation:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>if all works out, you should be able to see the true relation (note that the constant offset might not be exactly 1, since it is allowed to round to the nearest integer). You can get the sympy version of the best equation with:</p> <pre><code>model.sympy()\n</code></pre>"},{"location":"examples/#8-complex-numbers","title":"8. Complex numbers","text":"<p>PySR can also search for complex-valued expressions. Simply pass data with a complex datatype (e.g., <code>np.complex128</code>), and PySR will automatically search for complex-valued expressions:</p> <pre><code>import numpy as np\n\nX = np.random.randn(100, 1) + 1j * np.random.randn(100, 1)\ny = (1 + 2j) * np.cos(X[:, 0] * (0.5 - 0.2j))\n\nmodel = PySRRegressor(\n    binary_operators=[\"+\", \"-\", \"*\"], unary_operators=[\"cos\"], niterations=100,\n)\n\nmodel.fit(X, y)\n</code></pre> <p>You can see that all of the learned constants are now complex numbers. We can get the sympy version of the best equation with:</p> <pre><code>model.sympy()\n</code></pre> <p>We can also make predictions normally, by passing complex data:</p> <pre><code>model.predict(X, -1)\n</code></pre> <p>to make predictions with the most accurate expression.</p>"},{"location":"examples/#9-custom-objectives","title":"9. Custom objectives","text":"<p>You can also pass a custom objectives as a snippet of Julia code, which might include symbolic manipulations or custom functional forms. These do not even need to be differentiable! First, let's look at the default objective used (a simplified version, without weights and with mean square error), so that you can see how to write your own:</p> <pre><code>function default_objective(tree, dataset::Dataset{T,L}, options)::L where {T,L}\n    (prediction, completion) = eval_tree_array(tree, dataset.X, options)\n    if !completion\n        return L(Inf)\n    end\n\n    diffs = prediction .- dataset.y\n\n    return sum(diffs .^ 2) / length(diffs)\nend\n</code></pre> <p>Here, the <code>where {T,L}</code> syntax defines the function for arbitrary types <code>T</code> and <code>L</code>. If you have <code>precision=32</code> (default) and pass in regular floating point data, then both <code>T</code> and <code>L</code> will be equal to <code>Float32</code>. If you pass in complex data, then <code>T</code> will be <code>ComplexF32</code> and <code>L</code> will be <code>Float32</code> (since we need to return a real number from the loss function). But, you don't need to worry about this, just make sure to return a scalar number of type <code>L</code>.</p> <p>The <code>tree</code> argument is the current expression being evaluated. You can read about the <code>tree</code> fields here.</p> <p>For example, let's fix a symbolic form of an expression, as a rational function. i.e., \\(P(X)/Q(X)\\) for polynomials \\(P\\) and \\(Q\\).</p> <pre><code>objective = \"\"\"\nfunction my_custom_objective(tree, dataset::Dataset{T,L}, options) where {T,L}\n    # Require root node to be binary, so we can split it,\n    # otherwise return a large loss:\n    tree.degree != 2 &amp;&amp; return L(Inf)\n\n    P = tree.l\n    Q = tree.r\n\n    # Evaluate numerator:\n    P_prediction, flag = eval_tree_array(P, dataset.X, options)\n    !flag &amp;&amp; return L(Inf)\n\n    # Evaluate denominator:\n    Q_prediction, flag = eval_tree_array(Q, dataset.X, options)\n    !flag &amp;&amp; return L(Inf)\n\n    # Impose functional form:\n    prediction = P_prediction ./ Q_prediction\n\n    diffs = prediction .- dataset.y\n\n    return sum(diffs .^ 2) / length(diffs)\nend\n\"\"\"\n\nmodel = PySRRegressor(\n    niterations=100,\n    binary_operators=[\"*\", \"+\", \"-\"],\n    loss_function=objective,\n)\n</code></pre> <p>Warning: When using a custom objective like this that performs symbolic manipulations, many functionalities of PySR will not work, such as <code>.sympy()</code>, <code>.predict()</code>, etc. This is because the SymPy parsing does not know about how you are manipulating the expression, so you will need to do this yourself.</p> <p>Note how we did not pass <code>/</code> as a binary operator; it will just be implicit in the functional form.</p> <p>Let's generate an equation of the form \\(\\frac{x_0^2 x_1 - 2}{x_2^2 + 1}\\):</p> <pre><code>X = np.random.randn(1000, 3)\ny = (X[:, 0]**2 * X[:, 1] - 2) / (X[:, 2]**2 + 1)\n</code></pre> <p>Finally, let's fit:</p> <pre><code>model.fit(X, y)\n</code></pre> <p>Note that the printed equation is not the same as the evaluated equation, because the printing functionality does not know about the functional form.</p> <p>We can get the string format with:</p> <pre><code>model.get_best().equation\n</code></pre> <p>(or, you could use <code>model.equations_.iloc[-1].equation</code>)</p> <p>For me, this equation was:</p> <pre><code>(((2.3554819 + -0.3554746) - (x1 * (x0 * x0))) - (-1.0000019 - (x2 * x2)))\n</code></pre> <p>looking at the bracket structure of the equation, we can see that the outermost bracket is split at the <code>-</code> operator (note that we ignore the root operator in the evaluation, as we simply evaluated each argument and divided the result) into <code>((2.3554819 + -0.3554746) - (x1 * (x0 * x0)))</code> and <code>(-1.0000019 - (x2 * x2))</code>, meaning that our discovered equation is equal to: \\(\\frac{x_0^2 x_1 - 2.0000073}{x_2^2 + 1.0000019}\\), which is nearly the same as the true equation!</p>"},{"location":"examples/#10-dimensional-constraints","title":"10. Dimensional constraints","text":"<p>One other feature we can exploit is dimensional analysis. Say that we know the physical units of each feature and output, and we want to find an expression that is dimensionally consistent.</p> <p>We can do this as follows, using <code>DynamicQuantities.jl</code> to assign units, passing a string specifying the units for each variable. First, let's make some data on Newton's law of gravitation, using astropy for units:</p> <pre><code>import numpy as np\nfrom astropy import units as u, constants as const\n\nM = (np.random.rand(100) + 0.1) * const.M_sun\nm = 100 * (np.random.rand(100) + 0.1) * u.kg\nr = (np.random.rand(100) + 0.1) * const.R_earth\nG = const.G\n\nF = G * M * m / r**2\n</code></pre> <p>We can see the units of <code>F</code> with <code>F.unit</code>.</p> <p>Now, let's create our model. Since this data has such a large dynamic range, let's also create a custom loss function that looks at the error in log-space:</p> <pre><code>elementwise_loss = \"\"\"function loss_fnc(prediction, target)\n    scatter_loss = abs(log((abs(prediction)+1e-20) / (abs(target)+1e-20)))\n    sign_loss = 10 * (sign(prediction) - sign(target))^2\n    return scatter_loss + sign_loss\nend\n\"\"\"\n</code></pre> <p>Now let's define our model:</p> <pre><code>model = PySRRegressor(\n    binary_operators=[\"+\", \"-\", \"*\", \"/\"],\n    unary_operators=[\"square\"],\n    elementwise_loss=elementwise_loss,\n    complexity_of_constants=2,\n    maxsize=25,\n    niterations=100,\n    populations=50,\n    # Amount to penalize dimensional violations:\n    dimensional_constraint_penalty=10**5,\n)\n</code></pre> <p>and fit it, passing the unit information. To do this, we need to use the format of DynamicQuantities.jl.</p> <pre><code># Get numerical arrays to fit:\nX = pd.DataFrame(dict(\n    M=M.to(\"M_sun\").value,\n    m=m.to(\"kg\").value,\n    r=r.to(\"R_earth\").value,\n))\ny = F.value\n\nmodel.fit(\n    X,\n    y,\n    X_units=[\"Constants.M_sun\", \"kg\", \"Constants.R_earth\"],\n    y_units=\"kg * m / s^2\"\n)\n</code></pre> <p>You can observe that all expressions with a loss under our penalty are dimensionally consistent! (The <code>\"[\u22c5]\"</code> indicates free units in a constant, which can cancel out other units in the expression.) For example,</p> <pre><code>\"y[m s\u207b\u00b2 kg] = (M[kg] * 2.6353e-22[\u22c5])\"\n</code></pre> <p>would indicate that the expression is dimensionally consistent, with a constant <code>\"2.6353e-22[m s\u207b\u00b2]\"</code>.</p> <p>Note that this expression has a large dynamic range so may be difficult to find. Consider searching with a larger <code>niterations</code> if needed.</p> <p>Note that you can also search for exclusively dimensionless constants by settings <code>dimensionless_constants_only</code> to <code>true</code>.</p>"},{"location":"examples/#11-additional-features","title":"11. Additional features","text":"<p>For the many other features available in PySR, please read the Options section.</p>"},{"location":"interactive-docs/","title":"Interactive Reference","text":"<p>The following docs are interactive, and, based on your selections, will create a snippet of Python code at the bottom which you can execute locally. Clicking on each parameter's name will display a description. Note that this is an incomplete list of options; for the full list, see the API Reference.</p> <p></p>"},{"location":"operators/","title":"Operators","text":""},{"location":"operators/#pre-defined","title":"Pre-defined","text":"<p>First, note that pretty much any valid Julia function which takes one or two scalars as input, and returns on scalar as output, is likely to be a valid operator<sup>1</sup>. A selection of these and other valid operators are stated below.</p> <p>Binary</p> <ul> <li><code>+</code></li> <li><code>-</code></li> <li><code>*</code></li> <li><code>/</code></li> <li><code>^</code></li> <li><code>max</code></li> <li><code>min</code></li> <li><code>mod</code></li> <li><code>cond</code><ul> <li>Equal to <code>(x, y) -&gt; x &gt; 0 ? y : 0</code></li> </ul> </li> <li><code>greater</code><ul> <li>Equal to <code>(x, y) -&gt; x &gt; y ? 1 : 0</code></li> </ul> </li> <li><code>logical_or</code><ul> <li>Equal to <code>(x, y) -&gt; (x &gt; 0 || y &gt; 0) ? 1 : 0</code></li> </ul> </li> <li><code>logical_and</code><ul> <li>Equal to <code>(x, y) -&gt; (x &gt; 0 &amp;&amp; y &gt; 0) ? 1 : 0</code></li> </ul> </li> </ul> <p>Unary</p> <ul> <li><code>neg</code></li> <li><code>square</code></li> <li><code>cube</code></li> <li><code>exp</code></li> <li><code>abs</code></li> <li><code>log</code></li> <li><code>log10</code></li> <li><code>log2</code></li> <li><code>log1p</code></li> <li><code>sqrt</code></li> <li><code>sin</code></li> <li><code>cos</code></li> <li><code>tan</code></li> <li><code>sinh</code></li> <li><code>cosh</code></li> <li><code>tanh</code></li> <li><code>atan</code></li> <li><code>asinh</code></li> <li><code>acosh</code></li> <li><code>atanh_clip</code><ul> <li>Equal to <code>atanh(mod(x + 1, 2) - 1)</code></li> </ul> </li> <li><code>erf</code></li> <li><code>erfc</code></li> <li><code>gamma</code></li> <li><code>relu</code></li> <li><code>round</code></li> <li><code>floor</code></li> <li><code>ceil</code></li> <li><code>sign</code></li> </ul>"},{"location":"operators/#custom","title":"Custom","text":"<p>Instead of passing a predefined operator as a string, you can just define a custom function as Julia code. For example:</p> <pre><code>    PySRRegressor(\n        ...,\n        unary_operators=[\"myfunction(x) = x^2\"],\n        binary_operators=[\"myotherfunction(x, y) = x^2*y\"],\n        extra_sympy_mappings={\n            \"myfunction\": lambda x: x**2,\n            \"myotherfunction\": lambda x, y: x**2 * y,\n        },\n    )\n</code></pre> <p>Make sure that it works with <code>Float32</code> as a datatype (for default precision, or <code>Float64</code> if you set <code>precision=64</code>). That means you need to write <code>1.5f3</code> instead of <code>1.5e3</code>, if you write any constant numbers, or simply convert a result to <code>Float64(...)</code>.</p> <p>PySR expects that operators not throw an error for any input value over the entire real line from <code>-3.4e38</code> to <code>+3.4e38</code>. Thus, for invalid inputs, such as negative numbers to a <code>sqrt</code> function, you may simply return a <code>NaN</code> of the same type as the input. For example,</p> <pre><code>my_sqrt(x) = x &gt;= 0 ? sqrt(x) : convert(typeof(x), NaN)\n</code></pre> <p>would be a valid operator. The genetic algorithm will preferentially selection expressions which avoid any invalid values over the training dataset.</p> <ol> <li> <p>However, you will need to define a sympy equivalent in <code>extra_sympy_mapping</code> if you want to use a function not in the above list.\u00a0\u21a9</p> </li> </ol>"},{"location":"options/","title":"Features and Options","text":"<p>Some configurable features and options in <code>PySR</code> which you may find useful include:</p> <ul> <li>Selecting from the accuracy-complexity curve</li> <li>Operators</li> <li>Number of outer search iterations</li> <li>Number of inner search iterations</li> <li>Multi-processing</li> <li>Populations</li> <li>Data weighting</li> <li>Max complexity and depth</li> <li>Mini-batching</li> <li>Variable names</li> <li>Constraining use of operators</li> <li>Custom complexities</li> <li>LaTeX and SymPy</li> <li>Exporting to numpy, pytorch, and jax</li> <li>Loss functions</li> <li>Model loading</li> </ul> <p>These are described below. Also check out the tuning page for workflow tips.</p> <p>The program will output a pandas DataFrame containing the equations to <code>PySRRegressor.equations</code> containing the loss value and complexity.</p> <p>It will also dump to a csv at the end of every iteration, which is <code>.hall_of_fame_{date_time}.csv</code> by default. It also prints the equations to stdout.</p>"},{"location":"options/#model-selection","title":"Model selection","text":"<p>By default, <code>PySRRegressor</code> uses <code>model_selection='best'</code> which selects an equation from <code>PySRRegressor.equations_</code> using a combination of accuracy and complexity. You can also select <code>model_selection='accuracy'</code>.</p> <p>By printing a model (i.e., <code>print(model)</code>), you can see the equation selection with the arrow shown in the <code>pick</code> column.</p>"},{"location":"options/#operators","title":"Operators","text":"<p>A list of operators can be found on the operators page. One can define custom operators in Julia by passing a string:</p> <pre><code>PySRRegressor(niterations=100,\n    binary_operators=[\"mult\", \"plus\", \"special(x, y) = x^2 + y\"],\n    extra_sympy_mappings={'special': lambda x, y: x**2 + y},\n    unary_operators=[\"cos\"])\n</code></pre> <p>Now, the symbolic regression code can search using this <code>special</code> function that squares its left argument and adds it to its right. Make sure all passed functions are valid Julia code, and take one (unary) or two (binary) float32 scalars as input, and output a float32. This means if you write any real constants in your operator, like <code>2.5</code>, you have to write them instead as <code>2.5f0</code>, which defines it as <code>Float32</code>. Operators are automatically vectorized.</p> <p>One should also define <code>extra_sympy_mappings</code>, so that the SymPy code can understand the output equation from Julia, when constructing a useable function. This step is optional, but is necessary for the <code>lambda_format</code> to work.</p>"},{"location":"options/#iterations","title":"Iterations","text":"<p>This is the total number of generations that <code>pysr</code> will run for. I usually set this to a large number, and exit when I am satisfied with the equations.</p>"},{"location":"options/#cycles-per-iteration","title":"Cycles per iteration","text":"<p>Each cycle considers every 10-equation subsample (re-sampled for each individual 10, unless <code>fast_cycle</code> is set in which case the subsamples are separate groups of equations) a single time, producing one mutated equation for each. The parameter <code>ncycles_per_iteration</code> defines how many times this occurs before the equations are compared to the hall of fame, and new equations are migrated from the hall of fame, or from other populations. It also controls how slowly annealing occurs. You may find that increasing <code>ncycles_per_iteration</code> results in a higher cycles-per-second, as the head worker needs to reduce and distribute new equations less often, and also increases diversity. But at the same time, a smaller number it might be that migrating equations from the hall of fame helps each population stay closer to the best current equations.</p>"},{"location":"options/#processors","title":"Processors","text":"<p>One can adjust the number of workers used by Julia with the <code>procs</code> option. You should set this equal to the number of cores you want <code>pysr</code> to use.</p>"},{"location":"options/#populations","title":"Populations","text":"<p>By default, <code>populations=15</code>, but you can set a different number of populations with this option. More populations may increase the diversity of equations discovered, though will take longer to train. However, it is usually more efficient to have <code>populations&gt;procs</code>, as there are multiple populations running on each core.</p>"},{"location":"options/#weighted-data","title":"Weighted data","text":"<p>Here, we assign weights to each row of data using inverse uncertainty squared. We also use 10 processes for the search instead of the default.</p> <pre><code>sigma = ...\nweights = 1/sigma**2\n\nmodel = PySRRegressor(procs=10)\nmodel.fit(X, y, weights=weights)\n</code></pre>"},{"location":"options/#max-size","title":"Max size","text":"<p><code>maxsize</code> controls the maximum size of equation (number of operators, constants, variables). <code>maxdepth</code> is by default not used, but can be set to control the maximum depth of an equation. These will make processing faster, as longer equations take longer to test.</p> <p>One can warm up the maxsize from a small number to encourage PySR to start simple, by using the <code>warmupMaxsize</code> argument. This specifies that maxsize increases every <code>warmupMaxsize</code>.</p>"},{"location":"options/#batching","title":"Batching","text":"<p>One can turn on mini-batching, with the <code>batching</code> flag, and control the batch size with <code>batch_size</code>. This will make evolution faster for large datasets. Equations are still evaluated on the entire dataset at the end of each iteration to compare to the hall of fame, but only on a random subset during mutations and annealing.</p>"},{"location":"options/#variable-names","title":"Variable Names","text":"<p>You can pass a list of strings naming each column of <code>X</code> with <code>variable_names</code>. Alternatively, you can pass <code>X</code> as a pandas dataframe and the columns will be used as variable names. Make sure only alphabetical characters and <code>_</code> are used in these names.</p>"},{"location":"options/#constraining-use-of-operators","title":"Constraining use of operators","text":"<p>One can limit the complexity of specific operators with the <code>constraints</code> parameter. There is a \"maxsize\" parameter to PySR, but there is also an operator-level \"constraints\" parameter. One supplies a dict, like so:</p> <pre><code>constraints={'pow': (-1, 1), 'mult': (3, 3), 'cos': 5}\n</code></pre> <p>What this says is that: a power law \\(x^y\\) can have an expression of arbitrary (-1) complexity in the x, but only complexity 1 (e.g., a constant or variable) in the y. So \\((x_0 + 3)^{5.5}\\) is allowed, but \\(5.5^{x_0 + 3}\\) is not. I find this helps a lot for getting more interpretable equations. The other terms say that each multiplication can only have sub-expressions of up to complexity 3 (e.g., \\(5.0 + x_2\\)) in each side, and cosine can only operate on expressions of complexity 5 (e.g., \\(5.0 + x_2 exp(x_3)\\)).</p>"},{"location":"options/#custom-complexity","title":"Custom complexity","text":"<p>By default, all operators, constants, and instances of variables have a complexity of 1. The sum of the complexities of all terms is the total complexity of an expression. You may change this by configuring the options:</p> <ul> <li><code>complexity_of_operators</code> - pass a dictionary of <code>&lt;str&gt;: &lt;int&gt;</code> pairs   to change the complexity of each operator. If an operator is not   specified, it will have the default complexity of 1.</li> <li><code>complexity_of_constants</code> - supplying an integer will make all constants   have that complexity.</li> <li><code>complexity_of_variables</code> - supplying an integer will make all variables   have that complexity.</li> </ul>"},{"location":"options/#latex-and-sympy","title":"LaTeX and SymPy","text":"<p>After running <code>model.fit(...)</code>, you can look at <code>model.equations</code> which is a pandas dataframe. The <code>sympy_format</code> column gives sympy equations, and the <code>lambda_format</code> gives callable functions. You can optionally pass a pandas dataframe to the callable function, if you called <code>.fit</code> on a pandas dataframe as well.</p> <p>There are also some helper functions for doing this quickly.</p> <ul> <li><code>model.latex()</code> will generate a TeX formatted output of your equation.</li> <li><code>model.latex_table(indices=[2, 5, 8])</code> will generate a formatted LaTeX table including all the specified equations.</li> <li><code>model.sympy()</code> will return the SymPy representation.</li> <li><code>model.jax()</code> will return a callable JAX function combined with parameters (see below)</li> <li><code>model.pytorch()</code> will return a PyTorch model (see below).</li> </ul>"},{"location":"options/#exporting-to-numpy-pytorch-and-jax","title":"Exporting to numpy, pytorch, and jax","text":"<p>By default, the dataframe of equations will contain columns with the identifier <code>lambda_format</code>. These are simple functions which correspond to the equation, but executed with numpy functions. You can pass your <code>X</code> matrix to these functions just as you did to the <code>model.fit</code> call. Thus, this allows you to numerically evaluate the equations over different output.</p> <p>Calling <code>model.predict</code> will execute the <code>lambda_format</code> of the best equation, and return the result. If you selected <code>model_selection=\"best\"</code>, this will use an equation that combines accuracy with simplicity. For <code>model_selection=\"accuracy\"</code>, this will just look at accuracy.</p> <p>One can do the same thing for PyTorch, which uses code from sympytorch, and for JAX, which uses code from sympy2jax.</p> <p>Calling <code>model.pytorch()</code> will return a PyTorch module which runs the equation, using PyTorch functions, over <code>X</code> (as a PyTorch tensor). This is differentiable, and the parameters of this PyTorch module correspond to the learned parameters in the equation, and are trainable.</p> <pre><code>torch_model = model.pytorch()\ntorch_model(X)\n</code></pre> <p>Warning: If you are using custom operators, you must define <code>extra_torch_mappings</code> or <code>extra_jax_mappings</code> (both are <code>dict</code> of callables) to provide an equivalent definition of the functions. (At any time you can set these parameters or any others with <code>model.set_params</code>.)</p> <p>For JAX, you can equivalently call <code>model.jax()</code> This will return a dictionary containing a <code>'callable'</code> (a JAX function), and <code>'parameters'</code> (a list of parameters in the equation). You can execute this function with:</p> <pre><code>jax_model = model.jax()\njax_model['callable'](X, jax_model['parameters'])\n</code></pre> <p>Since the parameter list is a jax array, this therefore lets you also train the parameters within JAX (and is differentiable).</p>"},{"location":"options/#loss","title":"<code>loss</code>","text":"<p>The default loss is mean-square error, and weighted mean-square error. One can pass an arbitrary Julia string to define a custom loss, using, e.g., <code>elementwise_loss=\"myloss(x, y) = abs(x - y)^1.5\"</code>. For more details, see the Losses page for SymbolicRegression.jl.</p> <p>Here are some additional examples:</p> <p>abs(x-y) loss</p> <pre><code>PySRRegressor(..., elementwise_loss=\"f(x, y) = abs(x - y)^1.5\")\n</code></pre> <p>Note that the function name doesn't matter:</p> <pre><code>PySRRegressor(..., elementwise_loss=\"loss(x, y) = abs(x * y)\")\n</code></pre> <p>With weights:</p> <pre><code>model = PySRRegressor(..., elementwise_loss=\"myloss(x, y, w) = w * abs(x - y)\")\nmodel.fit(..., weights=weights)\n</code></pre> <p>Weights can be used in arbitrary ways:</p> <pre><code>model = PySRRegressor(..., weights=weights, elementwise_loss=\"myloss(x, y, w) = abs(x - y)^2/w^2\")\nmodel.fit(..., weights=weights)\n</code></pre> <p>Built-in loss (faster) (see losses). This one computes the L3 norm:</p> <pre><code>PySRRegressor(..., elementwise_loss=\"LPDistLoss{3}()\")\n</code></pre> <p>Can also uses these losses for weighted (weighted-average):</p> <pre><code>model = PySRRegressor(..., weights=weights, elementwise_loss=\"LPDistLoss{3}()\")\nmodel.fit(..., weights=weights)\n</code></pre>"},{"location":"options/#model-loading","title":"Model loading","text":"<p>PySR will automatically save a pickle file of the model state when you call <code>model.fit</code>, once before the search starts, and again after the search finishes. The filename will have the same base name as the input file, but with a <code>.pkl</code> extension. You can load the saved model state with:</p> <pre><code>model = PySRRegressor.from_file(pickle_filename)\n</code></pre> <p>If you have a long-running job and would like to load the model before completion, you can also do this. In this case, the model loading will use the <code>csv</code> file to load the equations, since the <code>csv</code> file is continually updated during the search. Once the search completes, the model including its equations will be saved to the pickle file, overwriting the existing version.</p>"},{"location":"papers/","title":"Research","text":"<p>Below is a showcase of papers which have used PySR to discover or rediscover a symbolic model. These are sorted by the date of release, with most recent papers at the top.</p> <p>If you have used PySR in your research, please submit a pull request to add your paper to this file.</p> <p></p> <p> Viktor Hru\u0161ka <sup>1</sup>, Aneta Furmanov\u00e1 <sup>1</sup>, Michal Bedna\u0159\u00edk <sup>1</sup> <p><sup>1</sup>Czech Technical University in Prague, Faculty of Electrical Engineering </p> <p>Abstract: Even though locally periodic structures have been studied for more than three decades, the known analytical expressions relating the waveguide geometry and the acoustic transmission are limited to a few special cases. Having an access to numerical model is a great opportunity for data-driven discovery. Our choice of cubic splines to parametrize the waveguide unit cell geometry offers enough variability for waveguide design. Using Webster equation for unit cell and Floquet\u2013Bloch theory for periodic structures, a dataset of numerical solutions was prepared. Employing the methods of physics-informed machine learning, we have extracted analytical formulae relating the waveguide geometry and the corresponding dispersion relation or directly the bandgap widths. The results contribute to the overall readability of the system and enable a deeper understanding of the underlying principles. Specifically, it allows for assessing the influence of the waveguide geometry, offering more efficient alternative to computationally demanding numerical optimization.</p> <p></p> <p> Ho Fung Tsoi <sup>1</sup>, Dylan Rankin <sup>1</sup>, Cecile Caillol <sup>2</sup>, Miles Cranmer <sup>3</sup>, Sridhara Dasu <sup>4</sup>, Javier Duarte <sup>5</sup>, Philip Harris <sup>6, 7</sup>, Elliot Lipeles <sup>1</sup>, Vladimir Loncar <sup>6, 8</sup> <p><sup>1</sup>University of Pennsylvania, <sup>2</sup>European Organization for Nuclear Research (CERN), <sup>3</sup>University of Cambridge, <sup>4</sup>University of Wisconsin-Madison, <sup>5</sup>University of California San Diego, <sup>6</sup>Massachusetts Institute of Technology, <sup>7</sup>Institute for Artificial Intelligence and Fundamental Interactions, <sup>8</sup>Institute of Physics Belgrade </p> <p>Abstract: We introduce SymbolFit, a framework that automates parametric modeling by using symbolic regression to perform a machine-search for functions that fit the data, while simultaneously providing uncertainty estimates in a single run. Traditionally, constructing a parametric model to accurately describe binned data has been a manual and iterative process, requiring an adequate functional form to be determined before the fit can be performed. The main challenge arises when the appropriate functional forms cannot be derived from first principles, especially when there is no underlying true closed-form function for the distribution. In this work, we address this problem by utilizing symbolic regression, a machine learning technique that explores a vast space of candidate functions without needing a predefined functional form, treating the functional form itself as a trainable parameter. Our approach is demonstrated in data analysis applications in high-energy physics experiments at the CERN Large Hadron Collider (LHC). We demonstrate its effectiveness and efficiency using five real proton-proton collision datasets from new physics searches at the LHC, namely the background modeling in resonance searches for high-mass dijet, trijet, paired-dijet, diphoton, and dimuon events. We also validate the framework using several toy datasets with one and more variables.</p> <p></p> <p> Miguel \u00c1ngel de Carvalho Servia <sup>1</sup>, Ilya Orson Sandoval <sup>1</sup>, King Kuok <sup>Mimi</sup> Hii <sup>1</sup>, Klaus Hellgardt <sup>1</sup>, Dongda Zhang <sup>2</sup>, Ehecatl Antonio del Rio Chanona <sup>1</sup> <p><sup>1</sup>Imperial College London, <sup>2</sup>University of Manchester </p> <p>Abstract: The industrialization of catalytic processes requires reliable kinetic models for their design, optimization and control. Mechanistic models require significant domain knowledge, while data-driven and hybrid models lack interpretability. Automated knowledge discovery methods, such as ALAMO (Automated Learning of Algebraic Models for Optimization), SINDy (Sparse Identification of Nonlinear Dynamics), and genetic programming, have gained popularity but suffer from limitations such as needing model structure assumptions, exhibiting poor scalability, and displaying sensitivity to noise. To overcome these challenges, we propose two methodological frameworks, ADoK-S and ADoK-W (Automated Discovery of Kinetic rate models using a Strong/Weak formulation of symbolic regression), for the automated generation of catalytic kinetic models using a robust criterion for model selection. We leverage genetic programming for model generation and a sequential optimization routine for model refinement. The frameworks are tested against three case studies of increasing complexity, demonstrating their ability to retrieve the underlying kinetic rate model with limited noisy data from the catalytic systems, showcasing their potential for chemical reaction engineering applications.</p> <p></p> <p> Christopher J. Soelistyo <sup>1</sup>, Alan R. Lowe <sup>1, 2</sup> <p><sup>1</sup>The Alan Turing Institute, <sup>2</sup>University College London </p> <p>Abstract: How can we find interpretable, domain-appropriate models of natural phenomena given some complex, raw data such as images? Can we use such models to derive scientific insight from the data? In this paper, we propose some methods for achieving this. In particular, we implement disentangled representation learning, sparse deep neural network training and symbolic regression, and assess their usefulness in forming interpretable models of complex image data. We demonstrate their relevance to the field of bioimaging using a well-studied test problem of classifying cell states in microscopy data. We find that such methods can produce highly parsimonious models that achieve ~98% of the accuracy of black-box benchmark models, with a tiny fraction of the complexity. We explore the utility of such interpretable models in producing scientific explanations of the underlying biological phenomenon.</p> <p></p> <p> Benjamin L. Davis <sup>1</sup>, Zehao Jin <sup>1</sup> <p><sup>1</sup>Center for Astrophysics and Space Science, New York University Abu Dhabi </p> <p>Abstract: Supermassive black holes (SMBHs) are tiny in comparison to the galaxies they inhabit, yet they manage to influence and coevolve along with their hosts. Evidence of this mutual development is observed in the structure and dynamics of galaxies and their correlations with black hole mass (\\(M_\\bullet\\)). For our study, we focus on relative parameters that are unique to only disk galaxies. As such, we quantify the structure of spiral galaxies via their logarithmic spiral-arm pitch angles (\\(\\phi\\)) and their dynamics through the maximum rotational velocities of their galactic disks (\\(v_\\mathrm{max}\\)). In the past, we have studied black hole mass scaling relations between \\(M_\\bullet\\) and \\(\\phi\\) or \\(v_\\mathrm{max}\\), separately. Now, we combine the three parameters into a trivariate \\(M_\\bullet\\)--\\(\\phi\\)--\\(v_\\mathrm{max}\\) relationship that yields best-in-class accuracy in prediction of black hole masses in spiral galaxies. Because most black hole mass scaling relations have been created from samples of the largest SMBHs within the most massive galaxies, they lack certainty when extrapolated to low-mass spiral galaxies. Thus, it is difficult to confidently use existing scaling relations when trying to identify galaxies that might harbor the elusive class of intermediate-mass black holes (IMBHs). Therefore, we offer our novel relationship as an ideal predictor to search for IMBHs and probe the low-mass end of the black hole mass function by utilizing spiral galaxies. Already with rotational velocities widely available for a large population of galaxies and pitch angles readily measurable from uncalibrated images, we expect that the \\(M_\\bullet\\)--\\(\\phi\\)--\\(v_\\mathrm{max}\\) fundamental plane will be a useful tool for estimating black hole masses, even at high redshifts.</p> <p></p> <p> Tanner Mengel <sup>1</sup>, Patrick Steffanic <sup>1</sup>, Charles Hughes <sup>1,2</sup>, Antonio Carlos Oliveira da Silva <sup>1,2</sup>, Christine Nattrass <sup>1</sup> <p><sup>1</sup>University of Tennessee, Knoxville, <sup>2</sup>Iowa State University of Science and Technology </p> <p>Abstract: Jet measurements in heavy ion collisions can provide constraints on the properties of the quark gluon plasma, but the kinematic reach is limited by a large, fluctuating background. We present a novel application of symbolic regression to extract a functional representation of a deep neural network trained to subtract background from jets in heavy ion collisions. We show that the deep neural network is approximately the same as a method using the particle multiplicity in a jet. This demonstrates that interpretable machine learning methods can provide insight into underlying physical processes.</p> <p></p> <p> Arthur Grundner <sup>1,2</sup>, Tom Beucler <sup>3</sup>, Pierre Gentine <sup>2,3</sup>, Veronika Eyring <sup>1,4</sup> <p><sup>1</sup>Institut f\u00fcr Physik der Atmosph\u00e4re, Deutsches Zentrum f\u00fcr Luft- und Raumfahrt, <sup>2</sup>Center for Learning the Earth with Artificial Intelligence And Physics, Columbia University, <sup>3</sup>Institute of Earth Surface Dynamics, University of Lausanne, <sup>4</sup>Institute of Environmental Physics, University of Bremen </p> <p>Abstract: A promising method for improving the representation of clouds in climate models, and hence climate projections, is to develop machine learning-based parameterizations using output from global storm-resolving models. While neural networks can achieve state-of-the-art performance, they are typically climate model-specific, require post-hoc tools for interpretation, and struggle to predict outside of their training distribution. To avoid these limitations, we combine symbolic regression, sequential feature selection, and physical constraints in a hierarchical modeling framework. This framework allows us to discover new equations diagnosing cloud cover from coarse-grained variables of global storm-resolving model simulations. These analytical equations are interpretable by construction and easily transferable to other grids or climate models. Our best equation balances performance and complexity, achieving a performance comparable to that of neural networks (\\(R^2=0.94\\)) while remaining simple (with only 13 trainable parameters). It reproduces cloud cover distributions more accurately than the Xu-Randall scheme across all cloud regimes (Hellinger distances \\(&lt;0.09\\)), and matches neural networks in condensate-rich regimes. When applied and fine-tuned to the ERA5 reanalysis, the equation exhibits superior transferability to new data compared to all other optimal cloud cover schemes. Our findings demonstrate the effectiveness of symbolic regression in discovering interpretable, physically-consistent, and nonlinear equations to parameterize cloud cover.</p> <p></p> <p> Yanzhang Li <sup>1</sup>, Hongyu Wang <sup>2</sup>, Yan Li <sup>1</sup>, Xiangzhi Bai <sup>2</sup>, Anhuai Lu <sup>1</sup> <p><sup>1</sup>Peking University, <sup>2</sup>Beihang University </p> <p>Abstract: Electron transfer is the most elementary process in nature, but the existing electron transfer rules are seldom applied to high-pressure situations, such as in the deep Earth. Here we show a deep learning model to obtain the electronegativity of 96 elements under arbitrary pressure, and a regressed unified formula to quantify its relationship with pressure and electronic configuration. The relative work function of minerals is further predicted by electronegativity, presenting a decreasing trend with pressure because of pressure-induced electron delocalization. Using the work function as the case study of electronegativity, it reveals that the driving force behind directional electron transfer results from the enlarged work function difference between compounds with pressure. This well explains the deep high-conductivity anomalies, and helps discover the redox reactivity between widespread Fe(II)-bearing minerals and water during ongoing subduction. Our results give an insight into the fundamental physicochemical properties of elements and their compounds under pressure</p> <p></p> <p> Digvijay Wadekar <sup>1</sup>, Leander Thiele <sup>2</sup>, J. Colin Hill <sup>3</sup>, Shivam Pandey <sup>4</sup>, Francisco Villaescusa-Navarro <sup>5</sup>, David N. Spergel <sup>5</sup>, Miles Cranmer <sup>2</sup>, Daisuke Nagai <sup>6</sup>, Daniel Angl\u00e9s-Alc\u00e1zar <sup>7</sup>, Shirley Ho <sup>5</sup>, Lars Hernquist <sup>8</sup> <p><sup>1</sup>Institute for Advanced Study, <sup>2</sup>Princeton University, <sup>3</sup>Columbia University, <sup>4</sup>University of Pennsylvania, <sup>5</sup>Flatiron Institute, <sup>6</sup>Yale University, <sup>7</sup>University of Connecticut, <sup>8</sup>Harvard University </p> <p>Abstract: Ionized gas in the halo circumgalactic medium leaves an imprint on the cosmic microwave background via the thermal Sunyaev-Zeldovich (tSZ) effect. Feedback from active galactic nuclei (AGN) and supernovae can affect the measurements of the integrated tSZ flux of halos (\\(Y_{SZ}\\)) and cause its relation with the halo mass (\\(Y_{SZ}-M\\)) to deviate from the self-similar power-law prediction of the virial theorem. We perform a comprehensive study of such deviations using CAMELS, a suite of hydrodynamic simulations with extensive variations in feedback prescriptions. We use a combination of two machine learning tools (random forest and symbolic regression) to search for analogues of the \\(Y-M\\) relation which are more robust to feedback processes for low masses (\\(M \\leq 10^{14} M_{\\odot}/h\\)); we find that simply replacing \\(Y \\rightarrow Y(1+M_\\ast/M_{\\text{gas}})\\) in the relation makes it remarkably self-similar. This could serve as a robust multiwavelength mass proxy for low-mass clusters and galaxy groups. Our methodology can also be generally useful to improve the domain of validity of other astrophysical scaling relations. We also forecast that measurements of the Y-M relation could provide percent-level constraints on certain combinations of feedback parameters and/or rule out a major part of the parameter space of supernova and AGN feedback models used in current state-of-the-art hydrodynamic simulations. Our results can be useful for using upcoming SZ surveys (e.g. SO, CMB-S4) and galaxy surveys (e.g. DESI and Rubin) to constrain the nature of baryonic feedback. Finally, we find that the an alternative relation, \\(Y-M_{\\ast}\\), provides complementary information on feedback than \\(Y-M\\).</p> <p></p> <p> Sergiy Verstyuk <sup>1</sup>, Michael R. Douglas <sup>1</sup> <p><sup>1</sup>Harvard University </p> <p>Abstract: Machine learning (ML) is becoming more and more important throughout the mathematical and theoretical sciences. In this work we apply modern ML methods to gravity models of pairwise interactions in international economics. We explain the formulation of graphical neural networks (GNNs), models for graph-structured data that respect the properties of exchangeability and locality. GNNs are a natural and theoretically appealing class of models for international trade, which we demonstrate empirically by fitting them to a large panel of annual-frequency country-level data. We then use a symbolic regression algorithm to turn our fits into interpretable models with performance comparable to state of the art hand-crafted models motivated by economic theory. The resulting symbolic models contain objects resembling market access functions, which were developed in modern structural literature, but in our analysis arise ab initio without being explicitly postulated. Along the way, we also produce several model-consistent and model-agnostic ML-based measures of bilateral trade accessibility.</p> <p></p> <p> Pablo Lemos <sup>1,2</sup>, Niall Jeffrey <sup>3,2</sup>, Miles Cranmer <sup>4</sup>, Shirley Ho <sup>4,5,6,7</sup>, Peter Battaglia <sup>8</sup> <p><sup>1</sup>University of Sussex, <sup>2</sup>University College London, <sup>3</sup>ENS, <sup>4</sup>Princeton University, <sup>5</sup>Flatiron Institute, <sup>6</sup>Carnegie Mellon University, <sup>7</sup>New York University, <sup>8</sup>DeepMind </p> <p>Abstract: We present an approach for using machine learning to automatically discover the governing equations and hidden properties of real physical systems from observations. We train a \"graph neural network\" to simulate the dynamics of our solar system's Sun, planets, and large moons from 30 years of trajectory data. We then use symbolic regression to discover an analytical expression for the force law implicitly learned by the neural network, which our results showed is equivalent to Newton's law of gravitation. The key assumptions that were required were translational and rotational equivariance, and Newton's second and third laws of motion. Our approach correctly discovered the form of the symbolic force law. Furthermore, our approach did not require any assumptions about the masses of planets and moons or physical constants. They, too, were accurately inferred through our methods. Though, of course, the classical law of gravitation has been known since Isaac Newton, our result serves as a validation that our method can discover unknown laws and hidden properties from observed data. More broadly this work represents a key step toward realizing the potential of machine learning for accelerating scientific discovery.</p> <p></p> <p> Patrick Kidger <sup>1</sup> <p><sup>1</sup>University of Oxford </p> <p>Abstract: The conjoining of dynamical systems and deep learning has become a topic of great interest. In particular, neural differential equations (NDEs) demonstrate that neural networks and differential equation are two sides of the same coin. Traditional parameterised differential equations are a special case. Many popular neural network architectures, such as residual networks and recurrent networks, are discretisations. NDEs are suitable for tackling generative problems, dynamical systems, and time series (particularly in physics, finance, ...) and are thus of interest to both modern machine learning and traditional mathematical modelling. NDEs offer high-capacity function approximation, strong priors on model space, the ability to handle irregular data, memory efficiency, and a wealth of available theory on both sides. This doctoral thesis provides an in-depth survey of the field. Topics include: neural ordinary differential equations (e.g. for hybrid neural/mechanistic modelling of physical systems); neural controlled differential equations (e.g. for learning functions of irregular time series); and neural stochastic differential equations (e.g. to produce generative models capable of representing complex stochastic dynamics, or sampling from complex high-dimensional distributions). Further topics include: numerical methods for NDEs (e.g. reversible differential equations solvers, backpropagation through differential equations, Brownian reconstruction); symbolic regression for dynamical systems (e.g. via regularised evolution); and deep implicit models (e.g. deep equilibrium models, differentiable optimisation). We anticipate this thesis will be of interest to anyone interested in the marriage of deep learning with dynamical systems, and hope it will provide a useful reference for the current state of the art.</p> <p></p> <p> Digvijay Wadekar <sup>1</sup>, Leander Thiele <sup>2</sup>, Francisco Villaescusa-Navarro <sup>3</sup>, J. Colin Hill <sup>4</sup>, Miles Cranmer <sup>2</sup>, David N. Spergel <sup>3</sup>, Nicholas Battaglia <sup>5</sup>, Daniel Angl\u00e9s-Alc\u00e1zar <sup>6</sup>, Lars Hernquist <sup>7</sup>, Shirley Ho <sup>3</sup> <p><sup>1</sup>Institute for Advanced Study, <sup>2</sup>Princeton University, <sup>3</sup>Flatiron Institute, <sup>4</sup>Columbia University, <sup>5</sup>Cornell University, <sup>6</sup>University of Connecticut, <sup>7</sup>Harvard University </p> <p>Abstract: Complex systems (stars, supernovae, galaxies, and clusters) often exhibit low scatter relations between observable properties (e.g., luminosity, velocity dispersion, oscillation period, temperature). These scaling relations can illuminate the underlying physics and can provide observational tools for estimating masses and distances. Machine learning can provide a fast and systematic way to search for new scaling relations (or for simple extensions to existing relations) in abstract high-dimensional parameter spaces. We use a machine learning tool called symbolic regression (SR), which models the patterns in a given dataset in the form of analytic equations. We focus on the Sunyaev-Zeldovich flux-cluster mass relation (Y-M), the scatter in which affects inference of cosmological parameters from cluster abundance data. Using SR on the data from the IllustrisTNG hydrodynamical simulation, we find a new proxy for cluster mass which combines \\(Y_{SZ}\\) and concentration of ionized gas (cgas): \\(M \\propto Y_{\\text{conc}}^{3/5} \\equiv Y_{SZ}^{3/5} (1 - A c_\\text{gas})\\). Yconc reduces the scatter in the predicted M by ~ 20 - 30% for large clusters (\\(M &gt; 10^{14} M_{\\odot}/h\\)) at both high and low redshifts, as compared to using just \\(Y_{SZ}\\). We show that the dependence on cgas is linked to cores of clusters exhibiting larger scatter than their outskirts. Finally, we test Yconc on clusters from simulations of the CAMELS project and show that Yconc is robust against variations in cosmology, astrophysics, subgrid physics, and cosmic variance. Our results and methodology can be useful for accurate multiwavelength cluster mass estimation from current and upcoming CMB and X-ray surveys like ACT, SO, SPT, eROSITA and CMB-S4.</p> <p></p> <p> Ana Maria Delgado <sup>1</sup>, Digvijay Wadekar <sup>2,3</sup>, Boryana Hadzhiyska <sup>1</sup>, Sownak Bose <sup>1,7</sup>, Lars Hernquist <sup>1</sup>, Shirley Ho <sup>2,4,5,6</sup> <p><sup>1</sup>Center for Astrophysics | Harvard &amp; Smithsonian, <sup>2</sup>New York University, <sup>3</sup>Institute for Advanced Study, <sup>4</sup>Flatiron Institute, <sup>5</sup>Princeton University, <sup>6</sup>Carnegie Mellon University, <sup>7</sup>Durham University </p> <p>Abstract: To extract information from the clustering of galaxies on non-linear scales, we need to model the connection between galaxies and halos accurately and in a flexible manner. Standard halo occupation distribution (HOD) models make the assumption that the galaxy occupation in a halo is a function of only its mass, however, in reality, the occupation can depend on various other parameters including halo concentration, assembly history, environment, spin, etc. Using the IllustrisTNG hydrodynamic simulation as our target, we show that machine learning tools can be used to capture this high-dimensional dependence and provide more accurate galaxy occupation models. Specifically, we use a random forest regressor to identify which secondary halo parameters best model the galaxy-halo connection and symbolic regression to augment the standard HOD model with simple equations capturing the dependence on those parameters, namely the local environmental overdensity and shear, at the location of a halo. This not only provides insights into the galaxy-formation relationship but, more importantly, improves the clustering statistics of the modeled galaxies significantly. Our approach demonstrates that machine learning tools can help us better understand and model the galaxy-halo connection, and are therefore useful for galaxy formation and cosmology studies from upcoming galaxy surveys.</p> <p></p> <p> Anja Butter <sup>1</sup>, Tilman Plehn <sup>1</sup>, Nathalie Soybelman <sup>1</sup>, Johann Brehmer <sup>2</sup> <p><sup>1</sup>Institut fur Theoretische Physik, Universitat Heidelberg, <sup>2</sup>Center for Data Science, New York University </p> <p>Abstract: While neural networks offer an attractive way to numerically encode functions, actual formulas remain the language of theoretical particle physics. We show how symbolic regression trained on matrix-element information provides, for instance, optimal LHC observables in an easily interpretable form. We introduce the method using the effect of a dimension-6 coefficient on associated ZH production. We then validate it for the known case of CP-violation in weak-boson-fusion Higgs production, including detector effects.</p> <p></p> <p> Helen Shao <sup>1</sup>, Francisco Villaescusa-Navarro <sup>1,2</sup>, Shy Genel <sup>2,3</sup>, David N. Spergel <sup>2,1</sup>, Daniel Angles-Alcazar <sup>4,2</sup>, Lars Hernquist <sup>5</sup>, Romeel Dave <sup>6,7,8</sup>, Desika Narayanan <sup>9,10</sup>, Gabriella Contardo <sup>2</sup>, Mark Vogelsberger <sup>11</sup> <p><sup>1</sup>Princeton University, <sup>2</sup>Flatiron Institute, <sup>3</sup>Columbia University, <sup>4</sup>University of Connecticut, <sup>5</sup>Center for Astrophysics | Harvard &amp; Smithsonian, <sup>6</sup>University of Edinburgh, <sup>7</sup>University of the Western Cape, <sup>8</sup>South African Astronomical Observatories, <sup>9</sup>University of Florida, <sup>10</sup>University of Florida Informatics Institute, <sup>11</sup>MIT </p> <p>Abstract: We use a generic formalism designed to search for relations in high-dimensional spaces to determine if the total mass of a subhalo can be predicted from other internal properties such as velocity dispersion, radius, or star-formation rate. We train neural networks using data from the Cosmology and Astrophysics with MachinE Learning Simulations (CAMELS) project and show that the model can predict the total mass of a subhalo with high accuracy: more than 99% of the subhalos have a predicted mass within 0.2 dex of their true value. The networks exhibit surprising extrapolation properties, being able to accurately predict the total mass of any type of subhalo containing any kind of galaxy at any redshift from simulations with different cosmologies, astrophysics models, subgrid physics, volumes, and resolutions, indicating that the network may have found a universal relation. We then use different methods to find equations that approximate the relation found by the networks and derive new analytic expressions that predict the total mass of a subhalo from its radius, velocity dispersion, and maximum circular velocity. We show that in some regimes, the analytic expressions are more accurate than the neural networks. We interpret the relation found by the neural network and approximated by the analytic equation as being connected to the virial theorem.</p> <p></p> <p> Jessica Craven <sup>1</sup>, Vishnu Jejjala <sup>1</sup>, Arjun Kar <sup>2</sup> <p><sup>1</sup>University of the Witwatersrand, <sup>2</sup>University of British Columbia </p> <p>Abstract: We present a simple phenomenological formula which approximates the hyperbolic volume of a knot using only a single evaluation of its Jones polynomial at a root of unity. The average error is just 2.86% on the first 1.7 million knots, which represents a large improvement over previous formulas of this kind. To find the approximation formula, we use layer-wise relevance propagation to reverse engineer a black box neural network which achieves a similar average error for the same approximation task when trained on 10% of the total dataset. The particular roots of unity which appear in our analysis cannot be written as e2\u03c0i/(k+2) with integer k; therefore, the relevant Jones polynomial evaluations are not given by unknot-normalized expectation values of Wilson loop operators in conventional SU(2) Chern-Simons theory with level k. Instead, they correspond to an analytic continuation of such expectation values to fractional level. We briefly review the continuation procedure and comment on the presence of certain Lefschetz thimbles, to which our approximation formula is sensitive, in the analytically continued Chern-Simons integration cycle.</p> <p></p> <p> Digvijay Wadekar <sup>1</sup>, Francisco Villaescusa-Navarro <sup>2,3</sup>, Shirley Ho <sup>2,3,4</sup>, Laurence Perreault-Levasseur <sup>3,5,6</sup> <p><sup>1</sup>New York University, <sup>2</sup>Princeton University, <sup>3</sup>Flatiron Institute, <sup>4</sup>Carnegie Mellon University, <sup>5</sup>Universit\u00e9 de Montr\u00e9al, <sup>6</sup>Mila </p> <p>Abstract: Upcoming 21cm surveys will map the spatial distribution of cosmic neutral hydrogen (HI) over unprecedented volumes. Mock catalogues are needed to fully exploit the potential of these surveys. Standard techniques employed to create these mock catalogs, like Halo Occupation Distribution (HOD), rely on assumptions such as the baryonic properties of dark matter halos only depend on their masses. In this work, we use the state-of-the-art magneto-hydrodynamic simulation IllustrisTNG to show that the HI content of halos exhibits a strong dependence on their local environment. We then use machine learning techniques to show that this effect can be 1) modeled by these algorithms and 2) parametrized in the form of novel analytic equations. We provide physical explanations for this environmental effect and show that ignoring it leads to underprediction of the real-space 21-cm power spectrum at k\u22730.05 h/Mpc by \u227310%, which is larger than the expected precision from upcoming surveys on such large scales. Our methodology of combining numerical simulations with machine learning techniques is general, and opens a new direction at modeling and parametrizing the complex physics of assembly bias needed to generate accurate mocks for galaxy and line intensity mapping surveys.</p>"},{"location":"papers/#analytical-formulae-for-design-of-one-dimensional-sonic-crystals-with-smooth-geometry-based-on-symbolic-regression","title":"Analytical formulae for design of one-dimensional sonic crystals with smooth geometry based on symbolic regression","text":""},{"location":"papers/#symbolfit-automatic-parametric-modeling-with-symbolic-regression","title":"SymbolFit: Automatic Parametric Modeling with Symbolic Regression","text":""},{"location":"papers/#the-automated-discovery-of-kinetic-rate-models-methodological-frameworks","title":"The automated discovery of kinetic rate models \u2013 methodological frameworks","text":""},{"location":"papers/#discovering-interpretable-models-of-scientific-image-data-with-deep-learning","title":"Discovering interpretable models of scientific image data with deep learning","text":""},{"location":"papers/#discovery-of-a-planar-black-hole-mass-scaling-relation-for-spiral-galaxies","title":"Discovery of a Planar Black Hole Mass Scaling Relation for Spiral Galaxies","text":""},{"location":"papers/#interpretable-machine-learning-methods-applied-to-jet-background-subtraction-in-heavy-ion-collisions","title":"Interpretable machine learning methods applied to jet background subtraction in heavy-ion collisions","text":""},{"location":"papers/#data-driven-equation-discovery-of-a-cloud-cover-parameterization","title":"Data-Driven Equation Discovery of a Cloud Cover Parameterization","text":""},{"location":"papers/#electron-transfer-rules-of-minerals-under-pressure-informed-by-machine-learning","title":"Electron Transfer Rules of Minerals under Pressure informed by Machine Learning","text":""},{"location":"papers/#the-sz-flux-mass-y-m-relation-at-low-halo-masses-improvements-with-symbolic-regression-and-strong-constraints-on-baryonic-feedback","title":"The SZ flux-mass (Y-M) relation at low halo masses: improvements with symbolic regression and strong constraints on baryonic feedback","text":""},{"location":"papers/#machine-learning-the-gravity-equation-for-international-trade","title":"Machine Learning the Gravity Equation for International Trade","text":""},{"location":"papers/#rediscovering-orbital-mechanics-with-machine-learning","title":"Rediscovering orbital mechanics with machine learning","text":""},{"location":"papers/#thesis-on-neural-differential-equations-section-61","title":"(Thesis) On Neural Differential Equations - Section 6.1","text":""},{"location":"papers/#augmenting-astrophysical-scaling-relations-with-machine-learning-application-to-reducing-the-sz-flux-mass-scatter","title":"Augmenting astrophysical scaling relations with machine learning: application to reducing the SZ flux-mass scatter","text":""},{"location":"papers/#modeling-the-galaxy-halo-connection-with-machine-learning","title":"Modeling the galaxy-halo connection with machine learning","text":""},{"location":"papers/#back-to-the-formula-lhc-edition","title":"Back to the Formula -- LHC Edition","text":""},{"location":"papers/#finding-universal-relations-in-subhalo-properties-with-artificial-intelligence","title":"Finding universal relations in subhalo properties with artificial intelligence","text":""},{"location":"papers/#disentangling-a-deep-learned-volume-formula","title":"Disentangling a deep learned volume formula","text":""},{"location":"papers/#modeling-assembly-bias-with-machine-learning-and-symbolic-regression","title":"Modeling assembly bias with machine learning and symbolic regression","text":""},{"location":"tuning/","title":"Tuning and Workflow Tips","text":"<p>I give a short guide below on how I like to tune PySR for my applications.</p> <p>First, my general tips would be to avoid using redundant operators, like how <code>pow</code> can do the same things as <code>square</code>, or how <code>-</code> (binary) and <code>neg</code> (unary) are equivalent. The fewer operators the better! Only use operators you need.</p> <p>When running PySR, I usually do the following:</p> <p>I run from IPython (Jupyter Notebooks don't work as well<sup>1</sup>) on the head node of a slurm cluster. Passing <code>cluster_manager=\"slurm\"</code> will make PySR set up a run over the entire allocation. I set <code>procs</code> equal to the total number of cores over my entire allocation.</p> <ol> <li>Use the default parameters.</li> <li>Use only the operators I think it needs and no more.</li> <li>Increase <code>populations</code> to <code>3*num_cores</code>.</li> <li>If my dataset is more than 1000 points, I either subsample it (low-dimensional and not much noise) or set <code>batching=True</code> (high-dimensional or very noisy, so it needs to evaluate on all the data).</li> <li>While on a laptop or single node machine, you might leave the default <code>ncycles_per_iteration</code>, on a cluster with ~100 cores I like to set <code>ncycles_per_iteration</code> to maybe <code>5000</code> or so, until the head node occupation is under <code>10%</code>. (A larger value means the workers talk less frequently to eachother, which is useful when you have many workers!)</li> <li>Set <code>constraints</code> and <code>nested_constraints</code> as strict as possible. These can help quite a bit with exploration. Typically, if I am using <code>pow</code>, I would set <code>constraints={\"pow\": (9, 1)}</code>, so that power laws can only have a variable or constant as their exponent. If I am using <code>sin</code> and <code>cos</code>, I also like to set <code>nested_constraints={\"sin\": {\"sin\": 0, \"cos\": 0}, \"cos\": {\"sin\": 0, \"cos\": 0}}</code>, so that sin and cos can't be nested, which seems to happen frequently. (Although in practice I would just use <code>sin</code>, since the search could always add a phase offset!)</li> <li>Set <code>maxsize</code> a bit larger than the final size you want. e.g., if you want a final equation of size <code>30</code>, you might set this to <code>35</code>, so that it has a bit of room to explore.</li> <li>I typically don't use <code>maxdepth</code>, but if I do, I set it strictly, while also leaving a bit of room for exploration. e.g., if you want a final equation limited to a depth of <code>5</code>, you might set this to <code>6</code> or <code>7</code>, so that it has a bit of room to explore.</li> <li>Set <code>parsimony</code> equal to about the minimum loss you would expect, divided by 5-10. e.g., if you expect the final equation to have a loss of <code>0.001</code>, you might set <code>parsimony=0.0001</code>.</li> <li>Set <code>weight_optimize</code> to some larger value, maybe <code>0.001</code>. This is very important if <code>ncycles_per_iteration</code> is large, so that optimization happens more frequently.</li> <li>Set <code>bumper</code> to <code>True</code>. This turns on bump allocation but is experimental. It should give you a nice 20% speedup.</li> <li>For final runs, after I have tuned everything, I typically set <code>niterations</code> to some very large value, and just let it run for a week until my job finishes (genetic algorithms tend not to converge, they can look like they settle down, but then find a new family of expression, and explore a new space). If I am satisfied with the current equations (which are visible either in the terminal or in the saved csv file), I quit the job early.</li> </ol> <p>Since I am running in IPython, I can just hit <code>q</code> and then <code>&lt;enter&gt;</code> to stop the job, tweak the hyperparameters, and then start the search again. I can also use <code>warm_start=True</code> if I wish to continue where I left off (though note that changing some parameters, like <code>maxsize</code>, are incompatible with warm starts).</p> <p>Some things I try out to see if they help:</p> <ol> <li>Play around with <code>complexity_of_operators</code>. Set operators you dislike (e.g., <code>pow</code>) to have a larger complexity.</li> <li>Try setting <code>adaptive_parsimony_scaling</code> a bit larger, maybe up to <code>1000</code>.</li> <li>Sometimes I try using <code>warmup_maxsize_by</code>. This is useful if you find that the search finds a very complex equation very quickly, and then gets stuck. It basically forces it to start at the simpler equations and build up complexity slowly.</li> <li>Play around with different losses:<ul> <li>I typically try <code>L2DistLoss()</code> and <code>L1DistLoss()</code>. L1 loss is more robust to outliers compared to L2 (L1 finds the median, while L2 finds the mean of a random variable), so is often a good choice for a noisy dataset.</li> <li>I might also provide the <code>weights</code> parameter to <code>fit</code> if there is some reasonable choice of weighting. For example, maybe I know the signal-to-noise of a particular row of <code>y</code> - I would set that SNR equal to the weights. Or, perhaps I do some sort of importance sampling, and weight the rows by importance.</li> </ul> </li> </ol> <p>Very rarely I might also try tuning the mutation weights, the crossover probability, or the optimization parameters. I never use <code>denoise</code> or <code>select_k_features</code> as I find they aren't very useful.</p> <p>For large datasets I usually just randomly sample ~1000 points or so. In case all the points matter, I might use <code>batching=True</code>.</p> <p>If I find the equations get very complex and I'm not sure if they are numerically precise, I might set <code>precision=64</code>.</p> <p>Once a run is finished, I use the <code>PySRRegressor.from_file</code> function to load the saved search in a different process (requires the pickle file, and possibly also the <code>.csv</code> file if you quit early). I can then explore the equations, convert them to LaTeX, and plot their output.</p>"},{"location":"tuning/#more-tips","title":"More Tips","text":"<p>You might also wish to explore the discussions page for more tips, and to see if anyone else has had similar questions. Be sure to also read through the reference.</p> <ol> <li> <p>Jupyter Notebooks are supported by PySR, but miss out on some useful features available in IPython and Python: the progress bar, and early stopping with \"q\". In Jupyter you cannot interrupt a search once it has started; you have to restart the kernel. See this issue for updates.\u00a0\u21a9</p> </li> </ol>"}]}